import os
import numpy as np
import matplotlib.pyplot as plt
import random
from collections import defaultdict
import copy

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F

# GPU ÏÑ§Ï†ï
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= "0,1,2"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('Device:', device)
torch.cuda.manual_seed_all(42)

def gaussian_kernel1d(kernel_size, sigma):
    x = torch.arange(kernel_size, dtype=torch.float32) - (kernel_size - 1) / 2.0
    kernel = torch.exp(-0.5 * (x / sigma) ** 2)
    kernel = kernel / kernel.sum()
    return kernel

def smooth_label_signal(signal, kernel_size=11, sigma=2.0):
    kernel = gaussian_kernel1d(kernel_size, sigma).to(signal.device)
    kernel = kernel.view(1, 1, kernel_size)
    padding = kernel_size // 2
    smoothed_signal = F.conv1d(signal, kernel, padding=padding)
    return smoothed_signal

def replace_zeros_with_random_torch(tensor, low=0.0, high=0.1, seed=42):
    torch.manual_seed(seed)
    mask = tensor <= 0.1
    random_vals = torch.empty(mask.sum(), device=tensor.device).uniform_(low, high)
    tensor[mask] = random_vals
    return tensor

# ==================== File-level Meta-Learning Dataset ====================
class FileBasedMetaDataset:
    """Í∞Å ÌååÏùºÏùÑ Î≥ÑÎèÑ taskÎ°ú Í¥ÄÎ¶¨ÌïòÎäî Meta-learning Dataset"""
    
    def __init__(self, file_dir, seq_len=512):
        self.file_dir = file_dir
        self.seq_len = seq_len
        self.needle_distances = ['0800', '1000']
        
        # Î™®Îì† ÌååÏùºÏùÑ Í∞úÎ≥Ñ taskÎ°ú Î°úÎìú
        self.all_tasks = self._load_all_file_tasks()
        
        print(f"üìã Total tasks loaded: {len(self.all_tasks)}")
        
    def _load_all_file_tasks(self):
        """Í∞Å ÌååÏùºÏùÑ Î≥ÑÎèÑ taskÎ°ú Î°úÎìú"""
        all_tasks = []
        
        for needle_dist in self.needle_distances:
            for file_idx in range(1, 21):  # 1~20 ÌååÏùº
                task_data = self._load_single_file_task(needle_dist, file_idx)
                if task_data is not None:
                    all_tasks.append(task_data)
                    
        return all_tasks
    
    def _load_single_file_task(self, needle_dist, file_idx):
        """Îã®Ïùº ÌååÏùºÏùÑ ÌïòÎÇòÏùò taskÎ°ú Î°úÎìú"""
        try:
            file_path = os.path.join(self.file_dir, f'T2D_{needle_dist}/SavedData_{file_idx:03d}.bin')
            
            with open(file_path, 'rb') as file1:
                data = np.frombuffer(file1.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                nor_x = data[0] - data[0][0]
                
                # Ïú†Ìö®ÏÑ± Ï≤¥ÌÅ¨
                if np.isnan(nor_x).any() or np.isinf(nor_x).any():
                    return None
                    
                indices = np.where(data[4] == 1)[0]
                if len(indices) < 2:
                    return None
                
                # ÎùºÎ≤® ÏÉùÏÑ±
                array_size = len(data[4])
                label_array = np.full(array_size, 0, dtype=np.float32)
                label_array[indices[1]-20 : indices[1]+20] = 1
                
                # Ïä§Î¨¥Îî©
                label_tensor = torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                smoothed_signal = smooth_label_signal(label_tensor, kernel_size=10001, sigma=500.0)
                smoothed_signal = smoothed_signal.squeeze(0).squeeze(0)
                
                # Ï†ïÍ∑úÌôî
                max_val = smoothed_signal.max()
                if max_val > 1e-6:
                    smoothed_signal = smoothed_signal / max_val
                else:
                    smoothed_signal = torch.ones_like(smoothed_signal) * 0.01
                
                # ÌõÑÏ≤òÎ¶¨
                smoothed_signal = replace_zeros_with_random_torch(smoothed_signal, low=0.01, high=0.1)
                smoothed_signal = torch.clamp(smoothed_signal, 0.0, 1.0)
                
                # ÏãúÌÄÄÏä§ ÏÉùÏÑ±
                x_sequences = []
                y_sequences = []
                
                for j in range(self.seq_len, len(nor_x) - self.seq_len):
                    x_win = nor_x[j - self.seq_len : j]
                    
                    if np.isnan(x_win).any() or np.isinf(x_win).any():
                        continue
                    
                    y_win = smoothed_signal[j]
                    
                    if torch.isnan(y_win) or torch.isinf(y_win):
                        continue
                    
                    x_sequences.append(torch.tensor(x_win, dtype=torch.float32))
                    y_sequences.append(torch.tensor(y_win.item(), dtype=torch.float32))
                
                if len(x_sequences) < 50:  # ÏµúÏÜå ÏãúÌÄÄÏä§ Ïàò Ï≤¥ÌÅ¨
                    return None
                    
                return {
                    'task_id': f"{needle_dist}_file_{file_idx:03d}",
                    'needle_dist': needle_dist,
                    'file_idx': file_idx,
                    'x_seq': x_sequences,
                    'y_seq': y_sequences,
                    'total_sequences': len(x_sequences)
                }
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading {needle_dist}_file_{file_idx:03d}: {e}")
            return None
    
    def sample_support_query(self, task_data, k_shot=5, query_size=15):
        """Îã®Ïùº taskÏóêÏÑú Support/Query set ÏÉòÌîåÎßÅ"""
        total_sequences = len(task_data['x_seq'])
        
        if total_sequences < k_shot + query_size:
            # ÏãúÌÄÄÏä§Í∞Ä Î∂ÄÏ°±ÌïòÎ©¥ ÎπÑÏú®Î°ú Ï°∞Ï†ï
            k_shot = max(1, total_sequences // 3)
            query_size = total_sequences - k_shot
        
        # ÎûúÎç§ ÏÉòÌîåÎßÅ
        indices = random.sample(range(total_sequences), k_shot + query_size)
        support_indices = indices[:k_shot]
        query_indices = indices[k_shot:]
        
        support_data = {
            'x': [task_data['x_seq'][i] for i in support_indices],
            'y': [task_data['y_seq'][i] for i in support_indices]
        }
        
        query_data = {
            'x': [task_data['x_seq'][i] for i in query_indices],
            'y': [task_data['y_seq'][i] for i in query_indices]
        }
        
        return support_data, query_data
    
    def get_cross_needle_split(self):
        """Cross-needle evaluationÏùÑ ÏúÑÌïú Î∂ÑÌï†"""
        needle_0800_tasks = [task for task in self.all_tasks if task['needle_dist'] == '0800']
        needle_1000_tasks = [task for task in self.all_tasks if task['needle_dist'] == '1000']
        
        return needle_0800_tasks, needle_1000_tasks
    
    def get_file_based_split(self, train_ratio=0.8):
        """ÌååÏùº Í∏∞Î∞ò train/test Î∂ÑÌï†"""
        random.shuffle(self.all_tasks)
        split_idx = int(len(self.all_tasks) * train_ratio)
        
        train_tasks = self.all_tasks[:split_idx]
        test_tasks = self.all_tasks[split_idx:]
        
        return train_tasks, test_tasks

# ==================== MAML Model ====================
class MAMLPunctureDetector(nn.Module):
    """Îπ†Î•∏ Ï†ÅÏùëÏùÑ ÏúÑÌïú Í≤ΩÎüâ Î™®Îç∏"""
    
    def __init__(self, input_size=1, hidden_size=128, num_layers=1, output_size=1):
        super(MAMLPunctureDetector, self).__init__()
        
        # üîß Îçî ÏûëÍ≥† Îπ†Î•∏ Î™®Îç∏ (ÌååÏùºÎãπ Ï†ÅÏùÄ Îç∞Ïù¥ÌÑ∞Ïù¥ÎØÄÎ°ú)
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.1)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 32) 
        self.fc3 = nn.Linear(32, output_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        
        fc1_out = F.relu(self.fc1(lstm_out))
        fc1_out = self.dropout(fc1_out)
        fc2_out = F.relu(self.fc2(fc1_out))
        
        return self.fc3(fc2_out)
    
    def clone(self):
        return copy.deepcopy(self)

# ==================== MAML Trainer ====================
class FileBasedMAMLTrainer:
    """ÌååÏùº Í∏∞Î∞ò MAML ÌïôÏäµ"""
    
    def __init__(self, model, meta_lr=0.001, inner_lr=0.01, inner_steps=3):
        self.model = model.to(device)
        self.meta_lr = meta_lr
        self.inner_lr = inner_lr
        self.inner_steps = inner_steps  # ÌååÏùºÎãπ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÅÏúºÎØÄÎ°ú Ï§ÑÏûÑ
        
        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=meta_lr)
        self.criterion = nn.BCEWithLogitsLoss()
        
    def inner_loop_adaptation(self, support_data, task_model):
        """Support setÏúºÎ°ú Îπ†Î•∏ Ï†ÅÏùë"""
        if len(support_data['x']) == 0:
            return task_model
            
        support_x = torch.stack(support_data['x']).to(device).unsqueeze(-1)
        support_y = torch.stack(support_data['y']).to(device)
        
        # Task-specific optimizer
        task_optimizer = optim.SGD(task_model.parameters(), lr=self.inner_lr)
        
        # Inner loop
        for step in range(self.inner_steps):
            task_optimizer.zero_grad()
            
            predictions = task_model(support_x).squeeze()
            loss = self.criterion(predictions, support_y)
            
            loss.backward()
            task_optimizer.step()
            
        return task_model
    
    def meta_update_step(self, batch_tasks):
        """Meta gradient Í≥ÑÏÇ∞ Î∞è ÏóÖÎç∞Ïù¥Ìä∏"""
        meta_loss = 0.0
        valid_tasks = 0
        
        self.meta_optimizer.zero_grad()
        
        for task_data in batch_tasks:
            try:
                # 1Ô∏è‚É£ Î™®Îç∏ Î≥µÏÇ¨
                task_model = self.model.clone()
                
                # 2Ô∏è‚É£ Support/Query Î∂ÑÌï†
                support_data, query_data = task_data
                
                if len(query_data['x']) == 0:
                    continue
                
                # 3Ô∏è‚É£ Inner loop Ï†ÅÏùë
                adapted_model = self.inner_loop_adaptation(support_data, task_model)
                
                # 4Ô∏è‚É£ Query set ÌèâÍ∞Ä
                query_x = torch.stack(query_data['x']).to(device).unsqueeze(-1)
                query_y = torch.stack(query_data['y']).to(device)
                
                query_predictions = adapted_model(query_x).squeeze()
                query_loss = self.criterion(query_predictions, query_y)
                
                meta_loss += query_loss
                valid_tasks += 1
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error in meta update: {e}")
                continue
        
        if valid_tasks > 0:
            meta_loss = meta_loss / valid_tasks
            meta_loss.backward()
            self.meta_optimizer.step()
            return meta_loss.item()
        else:
            return float('inf')
    
    def evaluate_on_task(self, task_data, k_shot=5):
        """Îã®Ïùº taskÏóêÏÑú few-shot ÏÑ±Îä• ÌèâÍ∞Ä"""
        try:
            support_data, query_data = task_data
            
            if len(query_data['x']) == 0:
                return 0.5, [], []
            
            # Î™®Îç∏ Ï†ÅÏùë
            task_model = self.model.clone()
            adapted_model = self.inner_loop_adaptation(support_data, task_model)
            
            # ÌèâÍ∞Ä
            adapted_model.eval()
            with torch.no_grad():
                query_x = torch.stack(query_data['x']).to(device).unsqueeze(-1)
                query_y = torch.stack(query_data['y']).to(device)
                
                predictions = adapted_model(query_x).squeeze()
                probas = torch.sigmoid(predictions)
                
                # AUC Í≥ÑÏÇ∞
                binary_labels = [1 if y.item() > 0.8 else 0 for y in query_y]
                if len(set(binary_labels)) > 1:
                    auc = roc_auc_score(binary_labels, probas.cpu().numpy())
                else:
                    auc = 0.5
                    
            return auc, probas.cpu().numpy(), query_y.cpu().numpy()
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error in evaluation: {e}")
            return 0.5, [], []

# ==================== Training Functions ====================
def train_file_based_maml():
    """ÌååÏùº Í∏∞Î∞ò Meta-learning ÌïôÏäµ"""
    
    print("üîÑ Loading file-based meta dataset...")
    meta_dataset = FileBasedMetaDataset(
        file_dir='/home/ibom002/dataset/Data_20250709',
        seq_len=512
    )
    
    if len(meta_dataset.all_tasks) < 10:
        print("‚ö†Ô∏è Insufficient tasks for meta-learning")
        return
    
    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
    model = MAMLPunctureDetector(
        input_size=1, 
        hidden_size=128, 
        num_layers=1, 
        output_size=1
    )
    
    trainer = FileBasedMAMLTrainer(
        model=model,
        meta_lr=0.001,
        inner_lr=0.01,
        inner_steps=3
    )
    
    # Train/Test Î∂ÑÌï†
    train_tasks, test_tasks = meta_dataset.get_file_based_split(train_ratio=0.8)
    
    print(f"üìä Train tasks: {len(train_tasks)}, Test tasks: {len(test_tasks)}")
    
    # Meta-training
    print("üöÄ Starting File-based Meta-Learning...")
    
    num_meta_epochs = 50
    tasks_per_batch = 4
    k_shot = 3  # ÌååÏùºÎãπ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÅÏúºÎØÄÎ°ú Ï§ÑÏûÑ
    query_size = 10
    
    best_avg_auc = 0.0
    
    for meta_epoch in range(num_meta_epochs):
        
        # Î∞∞Ïπò ÏÉòÌîåÎßÅ
        if len(train_tasks) < tasks_per_batch:
            batch_task_data = train_tasks
        else:
            batch_task_data = random.sample(train_tasks, tasks_per_batch)
        
        # Support/Query Î∂ÑÌï†
        batch_tasks = []
        for task in batch_task_data:
            support_data, query_data = meta_dataset.sample_support_query(
                task, k_shot=k_shot, query_size=query_size
            )
            batch_tasks.append((support_data, query_data))
        
        # Meta update
        meta_loss = trainer.meta_update_step(batch_tasks)
        
        # ÌèâÍ∞Ä (Îß§ 10 epochÎßàÎã§)
        if meta_epoch % 10 == 0:
            print(f"\nüìä Meta-Epoch {meta_epoch}")
            print(f"Meta Loss: {meta_loss:.6f}")
            
            # Test tasksÏóêÏÑú ÌèâÍ∞Ä
            test_aucs = []
            for test_task in test_tasks[:5]:  # 5Í∞ú taskÎßå ÌèâÍ∞Ä
                support_data, query_data = meta_dataset.sample_support_query(
                    test_task, k_shot=k_shot, query_size=15
                )
                auc, _, _ = trainer.evaluate_on_task((support_data, query_data), k_shot)
                test_aucs.append(auc)
            
            avg_auc = np.mean(test_aucs)
            print(f"Test AUC: {avg_auc:.4f} ¬± {np.std(test_aucs):.4f}")
            
            # Best model Ï†ÄÏû•
            if avg_auc > best_avg_auc:
                best_avg_auc = avg_auc
                torch.save(trainer.model.state_dict(), "./model/file_maml_best.pth")
                print(f"‚úÖ Best model saved! AUC: {best_avg_auc:.4f}")
    
    print(f"üéØ File-based Meta-Learning Completed! Best AUC: {best_avg_auc:.4f}")
    
    # Cross-needle evaluation
    print("\nüß™ Cross-needle Evaluation...")
    cross_needle_evaluation(trainer, meta_dataset)

def cross_needle_evaluation(trainer, meta_dataset):
    """Cross-needle generalization ÌèâÍ∞Ä"""
    
    needle_0800_tasks, needle_1000_tasks = meta_dataset.get_cross_needle_split()
    
    print(f"0800 tasks: {len(needle_0800_tasks)}, 1000 tasks: {len(needle_1000_tasks)}")
    
    # 0800ÏúºÎ°ú ÌïôÏäµ ‚Üí 1000ÏóêÏÑú ÌÖåÏä§Ìä∏
    print("\nüî¨ 0800 ‚Üí 1000 Generalization")
    test_cross_needle_performance(trainer, meta_dataset, needle_0800_tasks, needle_1000_tasks)
    
    # 1000ÏúºÎ°ú ÌïôÏäµ ‚Üí 0800ÏóêÏÑú ÌÖåÏä§Ìä∏  
    print("\nüî¨ 1000 ‚Üí 0800 Generalization")
    test_cross_needle_performance(trainer, meta_dataset, needle_1000_tasks, needle_0800_tasks)

def test_cross_needle_performance(trainer, meta_dataset, source_tasks, target_tasks):
    """Cross-needle ÏÑ±Îä• ÌÖåÏä§Ìä∏"""
    
    if len(target_tasks) == 0:
        print("‚ö†Ô∏è No target tasks available")
        return
    
    shot_sizes = [1, 3, 5]
    
    for k_shot in shot_sizes:
        aucs = []
        
        for target_task in target_tasks[:5]:  # 5Í∞úÎßå ÌÖåÏä§Ìä∏
            try:
                support_data, query_data = meta_dataset.sample_support_query(
                    target_task, k_shot=k_shot, query_size=10
                )
                
                auc, _, _ = trainer.evaluate_on_task((support_data, query_data), k_shot)
                aucs.append(auc)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error in cross-needle test: {e}")
                continue
        
        if aucs:
            avg_auc = np.mean(aucs)
            std_auc = np.std(aucs)
            print(f"{k_shot}-shot AUC: {avg_auc:.4f} ¬± {std_auc:.4f}")
        else:
            print(f"{k_shot}-shot: No valid results")

if __name__ == "__main__":
    train_file_based_maml()
