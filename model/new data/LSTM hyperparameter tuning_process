import os
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
import torch.nn.functional as F

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"  # Arrange GPU devices starting from 0
os.environ["CUDA_VISIBLE_DEVICES"]= "0, 1, 2"  # Set the GPUs 2 and 3 to use
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('Device:', device)
print('Current cuda device:', torch.cuda.current_device())
print('Count of using GPUs:', torch.cuda.device_count())
torch.cuda.manual_seed_all(42)

def gaussian_kernel1d(kernel_size, sigma):
    # kernel_sizeëŠ” í™€ìˆ˜ì—¬ì•¼ í•¨
    x = torch.arange(kernel_size, dtype=torch.float32) - (kernel_size - 1) / 2.0
    kernel = torch.exp(-0.5 * (x / sigma) ** 2)
    kernel = kernel / kernel.sum()  # í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
    return kernel


def smooth_label_signal(signal, kernel_size=11, sigma=2.0):
    kernel = gaussian_kernel1d(kernel_size, sigma).to(signal.device)
    kernel = kernel.view(1, 1, kernel_size)  # conv1dìš© shape
    padding = kernel_size // 2  # ì…ë ¥ê³¼ ì¶œë ¥ ê¸¸ì´ ë™ì¼í•˜ê²Œ ìœ ì§€
    smoothed_signal = F.conv1d(signal, kernel, padding=padding)
    return smoothed_signal

def replace_zeros_with_random_torch(tensor, low=0.0, high=0.1, seed=42):
    torch.manual_seed(seed)
    mask = tensor <= 0.1
    # maskê°€ Trueì¸ ìœ„ì¹˜ì— ëŒ€í•´ ëœë¤ ê°’ì„ ìƒì„±í•´ì„œ ëŒ€ì…
    random_vals = torch.empty(mask.sum(), device=tensor.device).uniform_(low, high)
    tensor[mask] = random_vals
    return tensor

def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []
    motor_x_seq = []
    needle_dis = ['0800', '1000']
    file_dir = '/home/ibom002/dataset/Data_20250709'

    for ind in needle_dis:
        for i in range_list:
            file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            with open(file_path, 'rb') as file1:
                data = np.frombuffer(file1.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                nor_x =  data[0] - data[0][0]
                indices = np.where(data[4] == 1)[0]
                array_size = len(data[4])
                label_array = np.full(array_size, 0, dtype=np.float32)
                label_array[indices[1]-20 : indices[1]+20] = 1
                
                if ind == '0600':
                    try:
                        # ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                        indices_high = np.where(data[0] > 3.05)[0]
                        indices_low = np.where(data[0] < -3.05)[0]
                        # 100 ì´ìƒ ì°¨ì´ë‚˜ëŠ” ì§€ì  ì°¾ê¸°
                        idx_diff_low = np.where(np.diff(indices_low) > 100)[0]

                        # ì‚­ì œí•  êµ¬ê°„ ì„¤ì •
                        start_1 = indices_high[0]  # ì‹œì‘ êµ¬ê°„ 1
                        end_1 = indices_low[idx_diff_low[0]]  # ë êµ¬ê°„ 1
                        start_2 = indices_low[idx_diff_low[0] + 1]  # ì‹œì‘ êµ¬ê°„ 2
                        end_2 = indices_high[-1]  # ë êµ¬ê°„ 2

                        # start_1 ~ end_2 êµ¬ê°„ì— 6.1ì„ ë”í•˜ê¸°
                        data[0][start_1:end_2 + 1] += 6.1  # í•´ë‹¹ êµ¬ê°„ì— 6.1 ì¶”ê°€

                        # í•„í„°ë§ì„ ìœ„í•œ ë§ˆìŠ¤í¬ ìƒì„±
                        indices = np.arange(len(data[0]))  # ì „ì²´ ì¸ë±ìŠ¤ ìƒì„±
                        mask = ~((indices >= start_1) & (indices <= end_1) | (indices >= start_2) & (indices <= end_2))

                        # íŠ¹ì • êµ¬ê°„ ì œê±°
                        x_data = data[0][mask]
                        nor_x =  x_data - data[0][0]
                        label_array = label_array[mask]
                    except Exception as e:
                        print(f"âš ï¸ ì˜ˆì™¸ ë°œìƒ (ind: {ind}, i: {i}) - {e}")
                        pass  # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                        
                        
                smoothed_signal = smooth_label_signal(torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0), kernel_size=10001, sigma=500.0)
                smoothed_signal = smoothed_signal / smoothed_signal.max()  # ìµœëŒ€ê°’ì„ 1ë¡œ ì¬ì •ê·œí™”
                smoothed_signal = smoothed_signal.squeeze(0).squeeze(0)
                smoothed_signal = replace_zeros_with_random_torch(smoothed_signal)
                
                for j in range(seq_len, len(nor_x) - seq_len):
                    x_win = nor_x[j - seq_len : j]
                    x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                    
                    y_win = smoothed_signal[j]
                    y_seq.append(torch.tensor(y_win, dtype=torch.float64))
                print(f'{ind} _ {i} file processed. Data length: {len(nor_x)}, Labels: {len(label_array)}')
                # motor_x_seq.append(data[2]) # motor input
                            
    
    print(f'total sequence : {len(x_seq)}')
    return x_seq, y_seq

class train_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0600','0800', '1000']
        self.seq_len = seq_len
        self.x_seq = []
        self.y_seq = []
        self.range_list = [i for i in range(1, 19)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]               
    
class val_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0600', '0800', '1000']
        self.seq_len = seq_len
        self.x_seq = []
        self.y_seq = []
        self.range_list = [i for i in range(18, 21)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]                 

file_dir = '/home/ibom002/dataset/Data_20250709'
seq_len = 512
batch_size = 512

train_ds = train_bin_dataset(file_dir, seq_len)
val_ds = val_bin_dataset(file_dir, seq_len)

print('train_ds_len', len(train_ds))
print('val_ds_len', len(val_ds))

# ì´ì§„í™”: 0.5ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš© (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
class_counts = [binary_labels.count(0), binary_labels.count(1)]
class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]  # ë°˜ë¹„ë¡€ ê°€ì¤‘ì¹˜

# ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì„¤ì •: ì´ì§„í™”í•œ ë ˆì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
weights = [class_weights[label] for label in binary_labels]
sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

# ë°ì´í„°ë¡œë”ì— ìƒ˜í”ŒëŸ¬ ì¶”ê°€
train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, shuffle=False, drop_last=True)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)

print('train_dl_len', len(train_dl))
print('val_dl_len', len(val_dl))

# âœ… LSTM ëª¨ë¸ (input_size=1 ìˆ˜ì •)
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMAnomalyDetector, self).__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size,512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥
        fc1_out = self.fc1(lstm_out)
        fc2_out = self.fc2(fc1_out)
        return self.fc3(fc2_out)  # ğŸ¯ BCEWithLogitsLossë¥¼ ì“°ë¯€ë¡œ sigmoid ì ìš© X
# âœ… train í•¨ìˆ˜ (ì¤‘ê°„ `val()` ì‹¤í–‰ ì¶”ê°€)
def train(train_dl, val_dl, model, criterion, optimizer, epoch, eval_every=1000):
    model.train()
    epoch_loss = 0

    for batch, (X, y) in enumerate(train_dl):
        optimizer.zero_grad()
        X, y = X.to(device).unsqueeze(-1), y.to(device).float().squeeze()
        pred = model(X).squeeze()
        loss = criterion(pred, y)  # ğŸ¯ BCEWithLogitsLoss ì‚¬ìš©
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        
        # ğŸ¯ ì¤‘ê°„ í‰ê°€ ì‹¤í–‰ (ë§¤ `eval_every` ë°°ì¹˜ë§ˆë‹¤ `val()` ì‹¤í–‰)
        if batch % eval_every == 0 and batch != 0:
            val_loss, val_auc, val_preds, val_labels = val(val_dl, model, criterion)
            print(f'ğŸ”¹ Epoch {epoch+1} | Batch {batch}: Validation AUC = {val_auc:.5f} | Validation loos = {val_loss:.5f}')
            model.train()
    return epoch_loss / len(train_dl)

# âœ… val í•¨ìˆ˜ (ROC-AUC ê³„ì‚° ì¶”ê°€)
def val(val_dl, model, criterion):
    model.eval()
    epoch_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch, (X, y) in enumerate(val_dl):
            X, y = X.to(device).unsqueeze(-1), y.to(device).float()
            pred = model(X).squeeze()  # raw logits
            loss = criterion(pred, y)
            epoch_loss += loss.item()
            probas = torch.sigmoid(pred)  # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
            all_preds.extend(probas.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    avg_loss = epoch_loss / len(val_dl)
    auc_score = roc_auc_score(all_labels, all_preds)
    print(f'ğŸ”¹ Validation Loss: {avg_loss:.10f}, ROC-AUC Score: {auc_score:.5f}')
    return avg_loss, auc_score, all_preds, all_labels

#1. lstm + early stopping

# âœ… ëª¨ë¸ ì„¤ì • ì˜µí‹°ë§ˆì´ì € & ì†ì‹¤ í•¨ìˆ˜
model = LSTMAnomalyDetector(input_size=1, hidden_size=512, num_layers=2, output_size=1)
model = nn.DataParallel(model).to(device)

optimizer = optim.Adam(model.parameters(), lr=0.00001)
criterion = nn.BCEWithLogitsLoss()

# âœ… ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ì„¤ì •
patience = 5         # ê°œì„ ë˜ì§€ ì•ŠëŠ” Epoch ìˆ˜ (ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€)
best_val_loss = float('inf')  # í˜„ì¬ê¹Œì§€ ê°€ì¥ ë‚®ì€ Validation Loss
early_stop_counter = 0        # ê°œì„ ë˜ì§€ ì•Šì€ Epoch ìˆ˜ ì¹´ìš´íŠ¸
min_delta = 0.0002            # Loss ê°œì„  ìµœì†Œ ê¸°ì¤€ (ë„ˆë¬´ ë¯¸ì„¸í•œ ì°¨ì´ëŠ” ë¬´ì‹œ)


# âœ… í•™ìŠµ ì‹¤í–‰ (ì¡°ê¸° ì¢…ë£Œ í¬í•¨)
num_epochs = 100
train_losses = []
val_losses = []
val_aucs = []

# validation ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ (ì‹œê°í™”ë¥¼ ìœ„í•´)
val_losses_history = []
val_aucs_history = []
val_preds_history = []   # ê° epochë³„ ì „ì²´ ì˜ˆì¸¡ê°’
val_labels_history = []  # ê° epochë³„ ì „ì²´ ì •ë‹µ

for epoch in range(num_epochs):
    print(f"\nğŸš€ Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
    model.train()
    epoch_loss = 0

    for batch, (X, y) in enumerate(train_dl):
        optimizer.zero_grad()
        X, y = X.to(device).unsqueeze(-1), y.to(device).float().squeeze()
        pred = model(X).squeeze()
        loss = criterion(pred, y)  # ğŸ¯ BCEWithLogitsLoss ì‚¬ìš©
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        train_loss = epoch_loss / len(train_dl)
    print("train loss: ", train_loss)
    
    model.eval()
    val_epoch_loss = 0
    # all_x_data = []
    all_preds = []
    # all_val_preds = []
    all_labels = []
    
    
    with torch.no_grad():
        for batch, (X, y) in enumerate(val_dl):
            X, val_labels = X.to(device).unsqueeze(-1), y.to(device).float()
            val_preds = model(X).squeeze()  # raw logits
            loss = criterion(val_preds, val_labels)
            val_epoch_loss += loss.item()
            probas = torch.sigmoid(val_preds)  # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
            X_data = X.cpu().numpy()
            # all_val_preds.extend(val_preds.cpu().numpy())
            # all_x_data.extend(X_data[-1])
            all_preds.extend(probas.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    val_loss = val_epoch_loss / len(val_dl)
    binary_labels = [1 if label > 0.8 else 0 for label in all_labels]
    val_auc = roc_auc_score(binary_labels, all_preds)
    
    print(f'ğŸ”¹ Validation Loss: {val_loss:.10f}, ROC-AUC Score: {val_auc:.5f}')
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_aucs.append(val_auc)
    
    # validation ê²°ê³¼ ì €ì¥ (ì‹œê°í™” ìš©)
    val_losses_history.append(val_loss)
    val_aucs_history.append(val_auc)
    val_preds_history.append(val_preds)
    val_labels_history.append(val_labels)
 
    # ğŸ¯ ì¡°ê¸° ì¢…ë£Œ ì²´í¬: val_loss ê¸°ì¤€
    if val_loss < (best_val_loss - min_delta):
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), "./model/best_model.pth")  # ğŸ¯ ëª¨ë¸ ì €ì¥
        print(f"âœ… Model saved! New best val_loss: {best_val_loss:.5f}")
    else:
        early_stop_counter += 1
        print(f"âš ï¸ Early stopping counter: {early_stop_counter}/{patience}")

    if early_stop_counter >= patience:
        print("â¹ Early stopping activated. Stopping training.")
        break  # ğŸ¯ í•™ìŠµ ì¤‘ë‹¨

print('ğŸ¯ Training Completed! Best Validation Loss:', best_val_loss)

# 2. kernel size = 2001, sigma 200 -> ì‹œê°„í¼ì§ ê°ì†Œ, ìŠ¬ë¼ì´ì‹± ìœˆë„ìš° ì •ë ¬ (ì˜ˆì¸¡ì‹œì ì¤‘ì‹¬)
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# âœ… Smoothed label ìƒì„± í•¨ìˆ˜ (ê°€ì •: ì´ë¯¸ ì •ì˜ë¼ ìˆìŒ)
def smooth_label_signal(x, kernel_size=2001, sigma=200.0):
    import torch.nn.functional as F
    import math
    half = kernel_size // 2
    x = x.float()

    # 1D Gaussian ì»¤ë„
    t = torch.arange(-half, half + 1, dtype=torch.float32)
    gauss = torch.exp(-t ** 2 / (2 * sigma ** 2))
    gauss /= gauss.sum()

    gauss = gauss.to(x.device)
    gauss = gauss.view(1, 1, -1)

    # padding í›„ convolution
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss)


# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []

    for ind in needle_dis:
        for i in range(range_list[0], range_list[1] + 1):
            file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            try:
                with open(file_path, 'rb') as f:
                    data = np.frombuffer(f.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                    force = data[0]
                    norm_force = force - force[0]
                    label_raw = data[4]

                    indices = np.where(label_raw == 1)[0]
                    if len(indices) < 2:
                        continue
                    second_puncture = indices[1]

                    label_array = np.zeros_like(label_raw)
                    label_array[second_puncture - 20 : second_puncture + 20] = 1

                    smoothed = smooth_label_signal(
                        torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0),
                        kernel_size=2001, sigma=200.0
                    ).squeeze().numpy()
                    smoothed = smoothed / smoothed.max()

                    for j in range(0, len(norm_force) - seq_len):
                        x_win = norm_force[j : j + seq_len]
                        y_idx = j + seq_len // 2
                        if y_idx >= len(smoothed):
                            break
                        y_val = smoothed[y_idx]
                        x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                        y_seq.append(torch.tensor(y_val, dtype=torch.float32))
            except Exception as e:
                print(f"âš ï¸ Error loading {file_path}: {e}")
    return x_seq, y_seq


# âœ… Dataset í´ë˜ìŠ¤
class bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len, mode='train'):
        self.needle_dis = [ '0800', '1000']
        self.seq_len = seq_len
        self.file_dir = file_dir
        if mode == 'train':
            self.range_list = (1, 17)
        else:
            self.range_list = (18, 20)
        self.x_seq, self.y_seq = Data_preprocessing(
            self.needle_dis, self.seq_len, self.file_dir, self.range_list
        )

    def __len__(self):
        return len(self.y_seq)

    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]


# âœ… ëª¨ë¸ êµ¬ì¡°
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        return self.fc3(self.fc2(self.fc1(lstm_out)))


# âœ… ë°ì´í„° ì¤€ë¹„
file_dir = '/home/ibom002/dataset/Data_20250709'
seq_len = 512
batch_size = 128

train_ds = bin_dataset(file_dir, seq_len, mode='train')
val_ds = bin_dataset(file_dir, seq_len, mode='val')

# âœ… ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìƒ˜í”Œë§
binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
class_counts = [binary_labels.count(0), binary_labels.count(1)]
class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]
weights = [class_weights[label] for label in binary_labels]
sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)


# âœ… ëª¨ë¸ & í•™ìŠµ ì„¤ì •
model = LSTMAnomalyDetector(input_size=1, hidden_size=512, num_layers=2, output_size=1)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-5)
criterion = nn.BCEWithLogitsLoss()

# âœ… í•™ìŠµ ë£¨í”„
num_epochs = 100
best_val_loss = float('inf')
early_stop_counter = 0
patience = 5
min_delta = 0.0002

for epoch in range(num_epochs):
    print(f"\nğŸš€ Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
    model.train()
    train_loss = 0
    for X, y in train_dl:
        X, y = X.to(device).unsqueeze(-1), y.to(device)
        pred = model(X).squeeze()
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_dl)
    print("Train Loss:", train_loss)

    model.eval()
    val_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X, y in val_dl:
            X, y = X.to(device).unsqueeze(-1), y.to(device)
            pred = model(X).squeeze()
            loss = criterion(pred, y)
            val_loss += loss.item()
            all_preds.extend(torch.sigmoid(pred).cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    val_loss /= len(val_dl)
    val_auc = roc_auc_score([1 if l > 0.8 else 0 for l in all_labels], all_preds)
    print(f"ğŸ”¹ Validation Loss: {val_loss:.6f}, ROC-AUC: {val_auc:.5f}")

    # âœ… ì¡°ê¸° ì¢…ë£Œ
    if val_loss < best_val_loss - min_delta:
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), './model/best_model.pth')
        print(f"âœ… Saved new best model with val_loss: {val_loss:.5f}")
    else:
        early_stop_counter += 1
        print(f"âš ï¸ Early stopping counter: {early_stop_counter}/{patience}")

    if early_stop_counter >= patience:
        print("â¹ Early stopping triggered.")
        break

print(f"\nğŸ¯ Training finished. Best Validation Loss: {best_val_loss:.5f}")

# 3. ì²«ë²ˆì§¸ puncture ë¬´ì‹œ, ë‘ë²ˆì§¸ puncture ì¤‘ì‹¬í•™ìŠµ, smoothing ë²”ìœ„ ê°ì†Œ, ì¤‘ì‹¬ì˜ˆì¸¡êµ¬ì¡°
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# âœ… Gaussian smoothing í•¨ìˆ˜ (ì •ë°€ ìŠ¤ë¬´ë”©)
def smooth_label_signal(x, kernel_size=101, sigma=10.0):
    import torch.nn.functional as F
    half = kernel_size // 2
    x = x.float()
    t = torch.arange(-half, half + 1, dtype=torch.float32)
    gauss = torch.exp(-t ** 2 / (2 * sigma ** 2))
    gauss /= gauss.sum()
    gauss = gauss.to(x.device).view(1, 1, -1)
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss)

# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []

    for ind in needle_dis:
        for i in range(range_list[0], range_list[1] + 1):
            file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            try:
                with open(file_path, 'rb') as f:
                    data = np.frombuffer(f.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                    force = data[0]
                    norm_force = force - force[0]
                    label_raw = data[4]

                    indices = np.where(label_raw == 1)[0]
                    if len(indices) < 2:
                        continue
                    second_puncture = indices[1]

                    label_array = np.zeros_like(label_raw)
                    label_array[second_puncture] = 1  # ì •í™•í•œ ìœ„ì¹˜ë§Œ 1

                    smoothed = smooth_label_signal(
                        torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0),
                        kernel_size=101, sigma=10.0
                    ).squeeze().numpy()
                    smoothed = smoothed / smoothed.max()

                    for j in range(0, len(norm_force) - seq_len):
                        x_win = norm_force[j : j + seq_len]
                        y_idx = j + seq_len // 2
                        if y_idx >= len(smoothed):
                            break
                        y_val = smoothed[y_idx]
                        x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                        y_seq.append(torch.tensor(y_val, dtype=torch.float32))
            except Exception as e:
                print(f"âš ï¸ Error loading {file_path}: {e}")
    return x_seq, y_seq

# âœ… Dataset í´ë˜ìŠ¤
class bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len, mode='train'):
        self.needle_dis = ['0800', '1000']
        self.seq_len = seq_len
        self.file_dir = file_dir
        self.range_list = (1, 17) if mode == 'train' else (18, 20)
        self.x_seq, self.y_seq = Data_preprocessing(
            self.needle_dis, self.seq_len, self.file_dir, self.range_list
        )

    def __len__(self):
        return len(self.y_seq)

    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]

# âœ… ëª¨ë¸ ì •ì˜
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        return self.fc3(self.fc2(self.fc1(lstm_out)))

# âœ… í•™ìŠµ ë°ì´í„° ì¤€ë¹„
file_dir = '/home/ibom002/dataset/Data_20250709'
seq_len = 512
batch_size = 128

train_ds = bin_dataset(file_dir, seq_len, mode='train')
val_ds = bin_dataset(file_dir, seq_len, mode='val')

# âœ… ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§
binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
class_counts = [binary_labels.count(0), binary_labels.count(1)]
class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]
weights = [class_weights[label] for label in binary_labels]
sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)

# âœ… ëª¨ë¸ í•™ìŠµ ì„¤ì •
model = LSTMAnomalyDetector(input_size=1, hidden_size=512, num_layers=2, output_size=1).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-5)
criterion = nn.BCEWithLogitsLoss()

# âœ… í•™ìŠµ ë£¨í”„
num_epochs = 50
best_val_loss = float('inf')
early_stop_counter = 0
patience = 7
min_delta = 0.0002

for epoch in range(num_epochs):
    print(f"\nğŸš€ Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
    model.train()
    train_loss = 0
    for X, y in train_dl:
        X, y = X.to(device).unsqueeze(-1), y.to(device)
        pred = model(X).squeeze()
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_dl)
    print("Train Loss:", train_loss)

    model.eval()
    val_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X, y in val_dl:
            X, y = X.to(device).unsqueeze(-1), y.to(device)
            pred = model(X).squeeze()
            loss = criterion(pred, y)
            val_loss += loss.item()
            all_preds.extend(torch.sigmoid(pred).cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    val_loss /= len(val_dl)
    val_auc = roc_auc_score([1 if l > 0.8 else 0 for l in all_labels], all_preds)
    print(f"ğŸ”¹ Validation Loss: {val_loss:.6f}, ROC-AUC: {val_auc:.5f}")

    if val_loss < best_val_loss - min_delta:
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), './model/best_model.pth')
        print(f"âœ… Saved new best model with val_loss: {val_loss:.5f}")
    else:
        early_stop_counter += 1
        print(f"âš ï¸ Early stopping counter: {early_stop_counter}/{patience}")

    if early_stop_counter >= patience:
        print("â¹ Early stopping triggered.")
        break

print(f"\nğŸ¯ Training finished. Best Validation Loss: {best_val_loss:.5f}")

# 4. 
# False-positive Loss â€“ y<0.1 êµ¬ê°„ì—ì„œ sigmoid(pred)>0.5ì¸ ìƒ˜í”Œì— ì¶”ê°€ í˜ë„í‹°ë¥¼ ì£¼ë©´, ëª¨ë¸ì´ â€œì—¬ê¸°ëŠ” ì ˆëŒ€ punctureê°€ ì•„ë‹ˆë‹¤â€ë¼ëŠ” ì‹ í˜¸ë¥¼ ë” ê°•í•˜ê²Œ í•™ìŠµí•©ë‹ˆë‹¤.
# â€“ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ ì•ë¶€ë¶„ì— ë†’ì€ ì¶œë ¥ì´ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ weightê°€ ì¡°ì •ë©ë‹ˆë‹¤.
# Weighted Sampler â€“ negative/positive ë¹„ìœ¨ì„ ê· ë“±í•˜ê²Œ ë½‘ì•„ ì£¼ë¯€ë¡œ, ì „ì²´ê°€ negativeì¸ êµ¬ê°„ì—ì„œì˜ í•™ìŠµ ë¹„ì¤‘ì´ ì˜¬ë¼ê°€ false-positive í™•ë¥ ì´ ìì—°ìŠ¤ëŸ½ê²Œ ê°ì†Œí•©ë‹ˆë‹¤.
# Asymmetric Label Smoothing â€“ ë¼ë²¨ ìŠ¤ë¬´ë”© ì»¤ë„ì„ ì•ìª½(ê³¼ê±°)ì— í¬ê²Œ, ë’¤ìª½(ë¯¸ë˜)ì— ì‘ê²Œ ì£¼ë©´ â€˜ì§„ì§œ puncture ì§í›„â€™ì—ë§Œ ë¼ë²¨ íš¨ê³¼ê°€ ì§‘ì¤‘ë˜ê³ , ê·¸ ì´ì „ êµ¬ê°„ì—” ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
# Post-processing (smoothing + peak filter)â€“ ê²€ì¦ ì‹œ gaussian_filter1dë¡œ ë¶€ë“œëŸ½ê²Œ í•œ ë’¤ find_peaks(height=0.5, distance=100)ë¥¼ ì“°ë©´, ì²« ë²ˆì§¸(ì˜ëª»ëœ) ì‘ì€ bumpëŠ” ë†’ì´ ê¸°ì¤€(0.5)ì´ë‚˜ ìµœì†Œ ê±°ë¦¬(100 ìŠ¤í…) ì¡°ê±´ì„ ì¶©ì¡±í•˜ì§€ ëª»í•´ í•„í„°ë§ë˜ê³ , ë‘ ë²ˆì§¸(ì§„ì§œ) í° peakë§Œ ê²€ì¶œë©ë‹ˆë‹¤.
# ì–¼ë¦¬ìŠ¤íƒ€í•‘ 5 -> 15

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

# post-processing ìš©
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# âœ… Asymmetric Gaussian smoothing í•¨ìˆ˜ (ë¹„ëŒ€ì¹­ ë¼ë²¨ ìŠ¤ë¬´ë”©)
def smooth_label_signal(x, left_sigma=300.0, right_sigma=100.0, kernel_size=1001):
    import torch.nn.functional as F
    half = kernel_size // 2
    t = torch.linspace(-1, 1, steps=kernel_size, device=x.device)
    left_mask = t < 0
    right_mask = ~left_mask
    left = torch.exp(-0.5 * (t[left_mask]**2) / (left_sigma**2))
    right = torch.exp(-0.5 * (t[right_mask]**2) / (right_sigma**2))
    gauss = torch.cat([left, right])
    gauss /= gauss.sum()
    gauss = gauss.view(1, 1, -1)

    x = x.float().unsqueeze(0).unsqueeze(0)  # [1,1,T]
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss).squeeze()

# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ê°œì„ ë¨)
def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq, y_seq, time_seq = [], [], []  # time_seq ì¶”ê°€
    for ind in needle_dis:
        for i in range(range_list[0], range_list[1] + 1):
            fp = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            try:
                data = np.fromfile(fp, dtype=np.float32).reshape(-1,5).T.copy()
                force = data[0]
                norm_force = force - force[0]
                raw_lbl = data[4]
                idxs = np.where(raw_lbl==1)[0]
                if len(idxs)<2: continue
                second = idxs[1]
                lbl_arr = np.zeros_like(raw_lbl, dtype=np.float32)
                lbl_arr[second] = 1.0
                sm = smooth_label_signal(torch.from_numpy(lbl_arr), left_sigma=300.0, right_sigma=100.0, kernel_size=1001).numpy()
                sm /= sm.max()
                
                # ì‹œê°„ ì •ë³´ ì¶”ê°€
                for j in range(len(norm_force)-seq_len):
                    x_seq.append(torch.tensor(norm_force[j:j+seq_len], dtype=torch.float32))
                    y_seq.append(torch.tensor(sm[j+seq_len//2], dtype=torch.float32))
                    time_seq.append(j+seq_len//2)  # ì¤‘ê°„ ì‹œì ì˜ ì‹œê°„ ì¸ë±ìŠ¤
            except Exception as e:
                print(f"âš ï¸ Error loading {fp}: {e}")
    return x_seq, y_seq, time_seq

# âœ… Dataset í´ë˜ìŠ¤ (ê°œì„ ë¨)
class bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len, mode='train'):
        self.needle_dis = ['0800','1000']
        self.seq_len = seq_len
        self.file_dir = file_dir
        self.range_list = (1,17) if mode=='train' else (18,20)
        self.x_seq, self.y_seq, self.time_seq = Data_preprocessing(self.needle_dis, self.seq_len, self.file_dir, self.range_list)
    def __len__(self): return len(self.y_seq)
    def __getitem__(self, idx): 
        return self.x_seq[idx], self.y_seq[idx], self.time_seq[idx]

# âœ… ê°œì„ ëœ ëª¨ë¸ ì •ì˜ (Attention ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€)
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)
        
        # Time-aware feature
        self.time_embedding = nn.Linear(1, hidden_size // 4)
        
        # Classification layers
        self.fc1 = nn.Linear(hidden_size + hidden_size // 4, 512)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(256, output_size)
        
    def forward(self, x, time_step=None):
        # LSTM
        h, _ = self.lstm(x)
        
        # Attention (ìµœê·¼ ì‹œì ì— ë” ì§‘ì¤‘)
        attn_out, _ = self.attention(h, h, h)
        
        # ë§ˆì§€ë§‰ ì¶œë ¥
        h_last = attn_out[:, -1, :]
        
        # Time embedding (ì‹œê°„ ì •ë³´ ì¶”ê°€)
        if time_step is not None:
            time_emb = self.time_embedding(time_step.unsqueeze(-1).float())
            h_last = torch.cat([h_last, time_emb], dim=1)
        
        # Classification
        h = torch.relu(self.fc1(h_last))
        h = self.dropout1(h)
        h = torch.relu(self.fc2(h))
        h = self.dropout2(h)
        return self.fc3(h)

# âœ… ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ (ì‹œê°„ ê¸°ë°˜ í˜ë„í‹°)
class TimePenaltyLoss(nn.Module):
    def __init__(self, base_criterion, early_penalty_weight=2.0, early_threshold=5000):
        super().__init__()
        self.base_criterion = base_criterion
        self.early_penalty_weight = early_penalty_weight
        self.early_threshold = early_threshold
    
    def forward(self, pred, target, time_step):
        # ê¸°ë³¸ ì†ì‹¤
        base_loss = self.base_criterion(pred, target)
        
        # ì´ˆê¸° ì‹œì ì—ì„œ ë†’ì€ ì˜ˆì¸¡ì— ëŒ€í•œ í˜ë„í‹°
        early_mask = time_step < self.early_threshold
        high_pred_mask = torch.sigmoid(pred) > 0.5
        early_fp_mask = early_mask & high_pred_mask & (target < 0.1)
        
        if early_fp_mask.any():
            early_penalty = self.early_penalty_weight * self.base_criterion(pred[early_fp_mask], target[early_fp_mask])
            base_loss += early_penalty
        
        return base_loss

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„°
file_dir   = '/home/ibom002/dataset/Data_20250709'
seq_len    = 512
batch_size = 128
num_epochs = 50

# âœ… ë°ì´í„° ë¡œë“œ & Sampler
train_ds = bin_dataset(file_dir, seq_len, mode='train')
val_ds   = bin_dataset(file_dir, seq_len, mode='val')
labels   = [1 if y.item()>0.8 else 0 for _,y,_ in train_ds]
counts   = [labels.count(0), labels.count(1)]
weights  = [1.0/counts[l] for l in labels]
sampler  = WeightedRandomSampler(weights, len(weights), replacement=True)
train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_dl   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)

# âœ… ëª¨ë¸Â·ì˜µí‹°ë§ˆì´ì €Â·ì†ì‹¤
model     = LSTMAnomalyDetector(1,512,2,1).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)
criterion = TimePenaltyLoss(nn.BCEWithLogitsLoss(), early_penalty_weight=3.0, early_threshold=5000)

# âœ… Early-stopping ì„¸íŒ…
best_metric       = 0.0
patience          = 15
min_delta         = 0.001
early_stop_counter= 0

for epoch in range(1, num_epochs+1):
    print(f"\nğŸš€ Epoch {epoch}/{num_epochs}")
    # --- Train ---
    model.train()
    train_loss = 0
    for X, y, time_step in train_dl:
        X, y, time_step = X.unsqueeze(-1).to(device), y.to(device), time_step.to(device)
        pred = model(X, time_step).squeeze()
        loss = criterion(pred, y, time_step)
        
        # ì¶”ê°€ì ì¸ False Positive ì–µì œ
        fp_mask = (y<0.1)&(torch.sigmoid(pred)>0.5)
        if fp_mask.any():
            loss += 0.5*nn.BCEWithLogitsLoss()(pred[fp_mask],y[fp_mask])
        
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_dl)
    print(f"  Train Loss: {train_loss:.6f}")

    # --- Validation ---
    model.eval()
    val_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X, y, time_step in val_dl:
            X, y, time_step = X.unsqueeze(-1).to(device), y.to(device), time_step.to(device)
            pred = model(X, time_step).squeeze()
            val_loss += nn.BCEWithLogitsLoss()(pred,y).item()
            all_preds.extend(torch.sigmoid(pred).cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    val_loss /= len(val_dl)

    # Postâ€processing + ì§€í‘œ
    raw = np.array(all_preds)
    smth= gaussian_filter1d(raw, sigma=10)
    smth_auc = roc_auc_score([1 if l>0.8 else 0 for l in all_labels], smth)
    print(f"  Val Loss: {val_loss:.6f}, SMTH ROC-AUC: {smth_auc:.5f}")

    # --- Early Stopping (SMTH ROC-AUC ê¸°ì¤€) ---
    if smth_auc > best_metric + min_delta:
        best_metric = smth_auc
        early_stop_counter = 0
        torch.save(model.state_dict(), './model/best2_model.pth')
        print(f"  âœ… Improved SMTH ROC-AUC to {smth_auc:.5f}, saved model.")
    else:
        early_stop_counter += 1
        print(f"  âš ï¸ No improvement ({early_stop_counter}/{patience})")
        if early_stop_counter >= patience:
            print("  â¹ Early stopping triggered.")
            break

print(f"\nğŸ¯ Training finished. Best SMTH ROC-AUC: {best_metric:.5f}")

# âœ… ì¶”ê°€: ì‹œê°„ë³„ ì˜ˆì¸¡ ë¶„ì„ í•¨ìˆ˜
def analyze_predictions_by_time(model, val_dl, device):
    model.eval()
    time_preds = []
    
    with torch.no_grad():
        for X, y, time_step in val_dl:
            X, y, time_step = X.unsqueeze(-1).to(device), y.to(device), time_step.to(device)
            pred = model(X, time_step).squeeze()
            pred_prob = torch.sigmoid(pred)
            
            for i in range(len(time_step)):
                time_preds.append({
                    'time': time_step[i].item(),
                    'prediction': pred_prob[i].item(),
                    'label': y[i].item()
                })
    
    return time_preds

# 5.  puncture ê·¼ì²˜ë§Œ focus
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# âœ… ì •ë°€í•œ Asymmetric Gaussian smoothing
def smooth_label_signal(x, left_sigma=100.0, right_sigma=30.0, kernel_size=401):
    """ë§¤ìš° ì¢ì€ ë²”ìœ„ì˜ ìŠ¤ë¬´ë”©ìœ¼ë¡œ ì •í™•í•œ puncture ì‹œì  í‘œí˜„"""
    half = kernel_size // 2
    t = torch.linspace(-1, 1, steps=kernel_size, device=x.device)
    left_mask = t < 0
    right_mask = ~left_mask
    left = torch.exp(-0.5 * (t[left_mask]**2) / (left_sigma**2))
    right = torch.exp(-0.5 * (t[right_mask]**2) / (right_sigma**2))
    gauss = torch.cat([left, right])
    gauss /= gauss.sum()
    gauss = gauss.view(1, 1, -1)

    x = x.float().unsqueeze(0).unsqueeze(0)
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss).squeeze()

# âœ… ê¸°ì¡´ ëª¨ë¸ ì •ì˜ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ)
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)
        self.time_embedding = nn.Linear(1, hidden_size // 4)
        self.fc1 = nn.Linear(hidden_size + hidden_size // 4, 512)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(256, output_size)
        
    def forward(self, x, time_step=None):
        h, _ = self.lstm(x)
        attn_out, _ = self.attention(h, h, h)
        h_last = attn_out[:, -1, :]
        
        if time_step is not None:
            time_emb = self.time_embedding(time_step.unsqueeze(-1).float())
            h_last = torch.cat([h_last, time_emb], dim=1)
        
        h = torch.relu(self.fc1(h_last))
        h = self.dropout1(h)
        h = torch.relu(self.fc2(h))
        h = self.dropout2(h)
        return self.fc3(h)

# âœ… í–¥ìƒëœ Focused Dataset (ì—¬ëŸ¬ íŒŒì¼ ì§€ì›)
class FocusedPunctureDataset(Dataset):
    def __init__(self, file_dir, needle_list=['0800', '1000'], file_ranges=None, seq_len=512, window=1500):
        """
        Args:
            file_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            needle_list: ë‹ˆë“¤ íƒ€ì… ë¦¬ìŠ¤íŠ¸
            file_ranges: íŒŒì¼ ë²”ìœ„ ë”•ì…”ë„ˆë¦¬ {'train': (1,17), 'val': (18,20)}
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
            window: puncture ì£¼ë³€ ìœˆë„ìš° í¬ê¸°
        """
        self.x, self.y, self.t, self.file_info = [], [], [], []
        
        if file_ranges is None:
            file_ranges = {'train': (1, 17), 'val': (18, 20)}
        
        for needle in needle_list:
            for mode, (start, end) in file_ranges.items():
                for file_idx in range(start, end + 1):
                    self._load_file(file_dir, needle, file_idx, seq_len, window)
        
        print(f"ğŸ“Š Focused Dataset loaded: {len(self.x)} samples")
        print(f"   High confidence samples: {sum(1 for y in self.y if y > 0.5)}")
    
    def _load_file(self, file_dir, needle, file_idx, seq_len, window):
        """ë‹¨ì¼ íŒŒì¼ ë¡œë“œ"""
        fp = os.path.join(file_dir, f'T2D_{needle}', f'SavedData_{file_idx:03d}.bin')
        try:
            data = np.fromfile(fp, dtype=np.float32).reshape(-1, 5).T.copy()
            force = data[0]
            norm_force = force - force[0]
            raw_lbl = data[4]
            idxs = np.where(raw_lbl == 1)[0]
            
            if len(idxs) < 2:
                return
            
            second = idxs[1]  # ì •í™•í•œ puncture ì‹œì 
            
            # ë§¤ìš° ì •ë°€í•œ ë¼ë²¨ ìƒì„±
            lbl_arr = np.zeros_like(raw_lbl, dtype=np.float32)
            lbl_arr[second] = 1.0
            sm = smooth_label_signal(torch.from_numpy(lbl_arr)).numpy()
            sm /= sm.max()
            
            # Puncture ê·¼ì²˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
            start = max(0, second - window)
            end = min(len(force), second + window)
            
            # ì˜¤ë²„ë©í•‘ ì‹œí€€ìŠ¤ ìƒì„± (ë” ì¡°ë°€í•˜ê²Œ)
            step_size = seq_len // 8  # ë” ë§ì€ ì˜¤ë²„ë©
            for j in range(start, end - seq_len, step_size):
                center_idx = j + seq_len // 2
                self.x.append(torch.tensor(norm_force[j:j+seq_len], dtype=torch.float32))
                self.y.append(torch.tensor(sm[center_idx], dtype=torch.float32))
                self.t.append(center_idx)
                self.file_info.append((needle, file_idx, j))
                
        except Exception as e:
            print(f"âš ï¸ Error loading {fp}: {e}")
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.t[idx]

# âœ… í–¥ìƒëœ Sharp Puncture Loss
class SharpPunctureLoss(nn.Module):
    def __init__(self, base_criterion=nn.BCEWithLogitsLoss(), 
                 early_threshold=8000, late_threshold=14000, 
                 penalty_weight=5.0, focus_weight=10.0):
        super().__init__()
        self.base = base_criterion
        self.early_threshold = early_threshold
        self.late_threshold = late_threshold
        self.penalty_weight = penalty_weight
        self.focus_weight = focus_weight
    
    def forward(self, pred, target, time_step):
        base_loss = self.base(pred, target)
        pred_prob = torch.sigmoid(pred)
        
        # 1. ì´ˆê¸° ì‹œì  False Positive ì–µì œ
        early_mask = time_step < self.early_threshold
        early_fp = early_mask & (pred_prob > 0.3) & (target < 0.1)
        if early_fp.any():
            early_penalty = self.penalty_weight * self.base(pred[early_fp], target[early_fp])
            base_loss += early_penalty
        
        # 2. í›„ê¸° ì‹œì  False Positive ì–µì œ
        late_mask = time_step > self.late_threshold
        late_fp = late_mask & (pred_prob > 0.3) & (target < 0.1)
        if late_fp.any():
            late_penalty = self.penalty_weight * self.base(pred[late_fp], target[late_fp])
            base_loss += late_penalty
        
        # 3. ì •í™•í•œ puncture ì‹œì  ì§‘ì¤‘ ê°•í™”
        high_target = target > 0.5
        if high_target.any():
            focus_loss = self.focus_weight * F.mse_loss(pred_prob[high_target], target[high_target])
            base_loss += focus_loss
        
        # 4. ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
        time_weight = torch.ones_like(time_step)
        puncture_region = (time_step >= 11000) & (time_step <= 13000)
        time_weight[puncture_region] = 2.0
        
        weighted_loss = base_loss * time_weight.mean()
        return weighted_loss

# âœ… í‰ê°€ í•¨ìˆ˜
def evaluate_focused_model(model, dataloader, device):
    model.eval()
    all_preds, all_labels, all_times = [], [], []
    
    with torch.no_grad():
        for X, y, t in dataloader:
            X, y, t = X.unsqueeze(-1).to(device), y.to(device), t.to(device)
            pred = model(X, t).squeeze()
            pred_prob = torch.sigmoid(pred)
            
            all_preds.extend(pred_prob.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
            all_times.extend(t.cpu().numpy())
    
    # ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬
    sorted_indices = np.argsort(all_times)
    sorted_preds = np.array(all_preds)[sorted_indices]
    sorted_labels = np.array(all_labels)[sorted_indices]
    sorted_times = np.array(all_times)[sorted_indices]
    
    # ìŠ¤ë¬´ë”© í›„ í”¼í¬ ê²€ì¶œ
    smoothed_preds = gaussian_filter1d(sorted_preds, sigma=3)
    peaks, properties = find_peaks(smoothed_preds, height=0.3, distance=50, prominence=0.1)
    
    # ê°€ì¥ ë†’ì€ í”¼í¬ ì°¾ê¸°
    if len(peaks) > 0:
        peak_heights = smoothed_preds[peaks]
        max_peak_idx = peaks[np.argmax(peak_heights)]
        predicted_time = sorted_times[max_peak_idx]
        confidence = peak_heights[np.argmax(peak_heights)]
    else:
        predicted_time = None
        confidence = 0.0
    
    # ì„±ëŠ¥ ì§€í‘œ
    binary_labels = (np.array(all_labels) > 0.5).astype(int)
    if len(set(binary_labels)) > 1:
        auc = roc_auc_score(binary_labels, all_preds)
        ap = average_precision_score(binary_labels, all_preds)
    else:
        auc, ap = 0.0, 0.0
    
    return {
        'auc': auc,
        'ap': ap,
        'predicted_time': predicted_time,
        'confidence': confidence,
        'peak_times': sorted_times[peaks] if len(peaks) > 0 else [],
        'peak_heights': peak_heights if len(peaks) > 0 else [],
        'predictions': sorted_preds,
        'times': sorted_times,
        'labels': sorted_labels
    }

# âœ… ë©”ì¸ Fine-tuning í•¨ìˆ˜
def fine_tune_puncture_model(file_dir, base_model_path='./model/best2_model.pth'):
    print("ğŸ”§ Starting Focused Puncture Fine-tuning...")
    
    # ëª¨ë¸ ë¡œë“œ
    model = LSTMAnomalyDetector(1, 512, 2, 1).to(device)
    
    if os.path.exists(base_model_path):
        model.load_state_dict(torch.load(base_model_path, map_location=device))
        print(f"âœ… Base model loaded from {base_model_path}")
    else:
        print("âš ï¸ Base model not found, starting from scratch")
    
    # Focused Dataset ìƒì„±
    train_ds = FocusedPunctureDataset(
        file_dir=file_dir,
        needle_list=['0800', '1000'],
        file_ranges={'train': (1, 17)},
        seq_len=512,
        window=2000  # ë” ë„“ì€ ìœˆë„ìš°ë¡œ ì»¨í…ìŠ¤íŠ¸ í™•ë³´
    )
    
    val_ds = FocusedPunctureDataset(
        file_dir=file_dir,
        needle_list=['0800', '1000'],
        file_ranges={'val': (18, 20)},
        seq_len=512,
        window=2000
    )
    
    # DataLoader ìƒì„±
    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)
    val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)
    
    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = optim.Adam(model.parameters(), lr=5e-7, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)
    criterion = SharpPunctureLoss(penalty_weight=8.0, focus_weight=15.0)
    
    # Fine-tuning loop
    best_confidence = 0.0
    num_epochs = 10
    
    for epoch in range(1, num_epochs + 1):
        print(f"\nğŸ”„ Fine-tune Epoch {epoch}/{num_epochs}")
        
        # Training
        model.train()
        epoch_loss = 0
        num_batches = 0
        
        for X, y, t in train_dl:
            X, y, t = X.unsqueeze(-1).to(device), y.to(device), t.to(device)
            
            optimizer.zero_grad()
            pred = model(X, t).squeeze()
            loss = criterion(pred, y, t)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        scheduler.step()
        avg_loss = epoch_loss / num_batches
        
        # Validation
        val_results = evaluate_focused_model(model, val_dl, device)
        
        print(f"  Loss: {avg_loss:.6f}")
        print(f"  AUC: {val_results['auc']:.4f}, AP: {val_results['ap']:.4f}")
        
        if val_results['predicted_time'] is not None:
            print(f"  ğŸ¯ Predicted puncture time: {val_results['predicted_time']:.0f}")
            print(f"  ğŸ“Š Confidence: {val_results['confidence']:.4f}")
            
            # ìµœê³  ì‹ ë¢°ë„ ëª¨ë¸ ì €ì¥
            if val_results['confidence'] > best_confidence:
                best_confidence = val_results['confidence']
                os.makedirs('./model', exist_ok=True)
                torch.save(model.state_dict(), './model/best2_model_finetuned_third.pth')
                print(f"  âœ… Best model saved with confidence: {best_confidence:.4f}")
        
        # ëª¨ë“  í”¼í¬ ì¶œë ¥
        if len(val_results['peak_times']) > 0:
            peaks_info = [(int(t), f"{h:.3f}") for t, h in 
                         zip(val_results['peak_times'], val_results['peak_heights'])]
            print(f"  ğŸ“ˆ All peaks: {peaks_info}")
    
    print(f"\nğŸ¯ Fine-tuning completed! Best confidence: {best_confidence:.4f}")
    
    # ìµœì¢… ëª¨ë¸ ë¡œë“œ ë° í‰ê°€
    model.load_state_dict(torch.load('./model/best2_model_finetuned_third.pth', map_location=device))
    final_results = evaluate_focused_model(model, val_dl, device)
    
    print(f"\nğŸ“Š Final Results:")
    print(f"  AUC: {final_results['auc']:.4f}")
    print(f"  AP: {final_results['ap']:.4f}")
    if final_results['predicted_time'] is not None:
        print(f"  ğŸ¯ Final predicted puncture time: {final_results['predicted_time']:.0f}")
        print(f"  ğŸ“Š Final confidence: {final_results['confidence']:.4f}")
    
    return model, final_results

# âœ… ì‹œê°í™” í•¨ìˆ˜
def visualize_fine_tuned_results(model, file_dir, needle='1000', file_idx=10):
    """Fine-tuned ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    fp = os.path.join(file_dir, f'T2D_{needle}', f'SavedData_{file_idx:03d}.bin')
    data = np.fromfile(fp, dtype=np.float32).reshape(-1, 5).T.copy()
    force = data[0]
    norm_force = force - force[0]
    raw_lbl = data[4]
    idxs = np.where(raw_lbl == 1)[0]
    
    if len(idxs) < 2:
        print("No puncture found in this file")
        return
    
    actual_puncture = idxs[1]
    
    # ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡
    model.eval()
    seq_len = 512
    predictions = []
    times = []
    
    with torch.no_grad():
        for i in range(0, len(norm_force) - seq_len, seq_len // 4):
            X = torch.tensor(norm_force[i:i+seq_len], dtype=torch.float32)
            X = X.unsqueeze(0).unsqueeze(-1).to(device)
            t = torch.tensor([i + seq_len // 2]).to(device)
            
            pred = model(X, t).squeeze()
            pred_prob = torch.sigmoid(pred).cpu().numpy()
            
            predictions.append(pred_prob)
            times.append(i + seq_len // 2)
    
    # ìŠ¤ë¬´ë”©
    smoothed_preds = gaussian_filter1d(predictions, sigma=5)
    
    # í”¼í¬ ê²€ì¶œ
    peaks, _ = find_peaks(smoothed_preds, height=0.3, distance=50)
    
    # ì‹œê°í™”
    plt.figure(figsize=(15, 10))
    
    # ìƒë‹¨: Force ì‹ í˜¸
    plt.subplot(2, 1, 1)
    plt.plot(norm_force, 'b-', label='Normalized Force', alpha=0.7)
    plt.axvline(x=actual_puncture, color='green', linestyle='--', linewidth=2, label=f'Actual Puncture ({actual_puncture})')
    plt.title(f'Force Signal - File {file_idx}')
    plt.xlabel('Time')
    plt.ylabel('Force')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í•˜ë‹¨: ì˜ˆì¸¡ ê²°ê³¼
    plt.subplot(2, 1, 2)
    plt.plot(times, predictions, 'r-', alpha=0.5, label='Raw Predictions')
    plt.plot(times, smoothed_preds, 'b-', linewidth=2, label='Smoothed Predictions')
    plt.axvline(x=actual_puncture, color='green', linestyle='--', linewidth=2, label=f'Actual Puncture ({actual_puncture})')
    
    # í”¼í¬ í‘œì‹œ
    if len(peaks) > 0:
        peak_times = [times[p] for p in peaks]
        peak_heights = [smoothed_preds[p] for p in peaks]
        plt.scatter(peak_times, peak_heights, color='red', s=100, zorder=5, label='Detected Peaks')
        
        # ê°€ì¥ ë†’ì€ í”¼í¬ ê°•ì¡°
        max_peak_idx = peaks[np.argmax(peak_heights)]
        max_peak_time = times[max_peak_idx]
        plt.scatter([max_peak_time], [smoothed_preds[max_peak_idx]], 
                   color='orange', s=200, zorder=6, label=f'Predicted Puncture ({max_peak_time:.0f})')
    
    plt.title('Fine-tuned Model Predictions')
    plt.xlabel('Time')
    plt.ylabel('Prediction Probability')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('./model/fine_tuned_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ê²°ê³¼ ì¶œë ¥
    if len(peaks) > 0:
        max_peak_time = times[peaks[np.argmax(peak_heights)]]
        error = abs(max_peak_time - actual_puncture)
        print(f"\nğŸ¯ Prediction Results:")
        print(f"  Actual puncture time: {actual_puncture}")
        print(f"  Predicted puncture time: {max_peak_time:.0f}")
        print(f"  Error: {error:.0f} samples")
        print(f"  Confidence: {max(peak_heights):.4f}")

# âœ… ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    file_dir = '/home/ibom002/dataset/Data_20250709'
    
    # Fine-tuning ì‹¤í–‰
    model, results = fine_tune_puncture_model(file_dir)
    
    # ê²°ê³¼ ì‹œê°í™”
    visualize_fine_tuned_results(model, file_dir, needle='1000', file_idx=10)
