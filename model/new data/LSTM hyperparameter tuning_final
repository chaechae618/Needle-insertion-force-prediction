import os
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
from scipy.signal import find_peaks

# GPU ì„¤ì •
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= "0,1,2"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('Device:', device)
torch.cuda.manual_seed_all(42)

def gaussian_kernel1d(kernel_size, sigma):
    x = torch.arange(kernel_size, dtype=torch.float32) - (kernel_size - 1) / 2.0
    kernel = torch.exp(-0.5 * (x / sigma) ** 2)
    kernel = kernel / kernel.sum()
    return kernel

def smooth_label_signal(signal, kernel_size=51, sigma=5.0):
    """ì ì ˆí•œ ìŠ¤ë¬´ë”© (ê³¼ë„í•˜ì§€ ì•Šê²Œ) - ê°œì„ : 51, 5.0ìœ¼ë¡œ ì¶•ì†Œ"""
    kernel = gaussian_kernel1d(kernel_size, sigma).to(signal.device)
    kernel = kernel.view(1, 1, kernel_size)
    padding = kernel_size // 2
    smoothed_signal = F.conv1d(signal, kernel, padding=padding)
    return smoothed_signal

def create_precise_label(data, puncture_indices, total_length, peak_width=30):
    """ì •í™•í•œ ë¼ë²¨ ìƒì„± - sharp peak ìƒì„±"""
    label_array = np.zeros(total_length, dtype=np.float32)
    
    if len(puncture_indices) >= 2:
        # ë‘ ë²ˆì§¸ punctureë§Œ ì‚¬ìš©
        center = puncture_indices[1]
        
        # Gaussian-like peak ìƒì„± (ë” sharpí•˜ê²Œ)
        for i in range(max(0, center - peak_width), min(total_length, center + peak_width)):
            distance = abs(i - center)
            # ë” sharpí•œ Gaussian
            intensity = np.exp(-0.5 * (distance / (peak_width/4)) ** 2)
            label_array[i] = intensity
            
    return label_array

def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []
    
    for ind in needle_dis:
        for i in range_list:
            try:
                file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
                with open(file_path, 'rb') as file1:
                    data = np.frombuffer(file1.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                    nor_x = data[0] - data[0][0]
                    
                    # NaN/Inf ì²´í¬
                    if np.isnan(nor_x).any() or np.isinf(nor_x).any():
                        continue
                        
                    indices = np.where(data[4] == 1)[0]
                    if len(indices) < 2:
                        continue
                    
                    # ğŸ”¥ ì •í™•í•œ ë¼ë²¨ ìƒì„± (sharp peak)
                    label_array = create_precise_label(data, indices, len(data[4]), peak_width=30)
                    
                    # 0600 ë°ì´í„° íŠ¹ë³„ ì²˜ë¦¬
                    if ind == '0600':
                        try:
                            indices_high = np.where(data[0] > 3.05)[0]
                            indices_low = np.where(data[0] < -3.05)[0]
                            idx_diff_low = np.where(np.diff(indices_low) > 100)[0]

                            if len(idx_diff_low) > 0 and len(indices_high) > 0:
                                start_1 = indices_high[0]
                                end_1 = indices_low[idx_diff_low[0]]
                                start_2 = indices_low[idx_diff_low[0] + 1]
                                end_2 = indices_high[-1]

                                data[0][start_1:end_2 + 1] += 6.1
                                indices_all = np.arange(len(data[0]))
                                mask = ~((indices_all >= start_1) & (indices_all <= end_1) | (indices_all >= start_2) & (indices_all <= end_2))

                                nor_x = data[0][mask] - data[0][0]
                                label_array = label_array[mask]
                        except:
                            pass
                    
                    if len(nor_x) == 0 or np.all(label_array == 0):
                        continue
                    
                    # ğŸ”¥ ê°€ë²¼ìš´ ìŠ¤ë¬´ë”©ë§Œ ì ìš© (peak ë³´ì¡´) - ê°œì„ : 51, 5.0 ì‚¬ìš©
                    label_tensor = torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                    smoothed_signal = smooth_label_signal(label_tensor, kernel_size=51, sigma=5.0)
                    smoothed_signal = smoothed_signal.squeeze()
                    
                    # ì •ê·œí™” (peakë¥¼ 1.0ìœ¼ë¡œ)
                    max_val = smoothed_signal.max()
                    if max_val > 1e-6:
                        smoothed_signal = smoothed_signal / max_val
                    else:
                        continue
                    
                    # ì•ˆì „ì„± ì²´í¬
                    if torch.isnan(smoothed_signal).any() or torch.isinf(smoothed_signal).any():
                        continue
                    
                    smoothed_signal = torch.clamp(smoothed_signal, 0.0, 1.0)
                    
                    # ì‹œí€€ìŠ¤ ìƒì„±
                    for j in range(seq_len, len(nor_x) - seq_len):
                        x_win = nor_x[j - seq_len : j]
                        
                        if np.isnan(x_win).any() or np.isinf(x_win).any():
                            continue
                        
                        y_win = smoothed_signal[j]
                        
                        if torch.isnan(y_win) or torch.isinf(y_win):
                            continue
                        
                        x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                        y_seq.append(torch.tensor(y_win.item(), dtype=torch.float32))
                    
                    print(f'{ind}_{i} processed. Sequences: {len(x_seq)}')
                    
            except Exception as e:
                print(f"Error processing {ind}_{i}: {e}")
                continue
    
    print(f'Total sequences: {len(x_seq)}')
    return x_seq, y_seq

# ğŸ”¥ ê°œì„ ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ - GELU, Attention, ì ì ˆí•œ Dropout
class ImprovedLSTMDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.1 if num_layers > 1 else 0)
        
        # ğŸ”¥ Self-attentionìœ¼ë¡œ peak ë¶€ë¶„ ê°•ì¡° (ì²˜ìŒë¶€í„° í™œì„±í™”)
        self.attention = nn.MultiheadAttention(hidden_size, 8, batch_first=True)
        self.norm1 = nn.LayerNorm(hidden_size)
        
        # ğŸ”¥ ê°œì„ ëœ ë¶„ë¥˜ê¸° - GELU ì‚¬ìš©, ë“œë¡­ì•„ì›ƒ ê°ì†Œ (10%)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.GELU(),  # ReLU ëŒ€ì‹  GELU ì‚¬ìš©
            nn.Dropout(0.1),  # 20% â†’ 10%ë¡œ ê°ì†Œ
            nn.Linear(256, 64),
            nn.GELU(),
            nn.Linear(64, output_size)
        )

    def forward(self, x):
        # LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(x)
        
        # Self-attentionìœ¼ë¡œ ì¤‘ìš”í•œ ë¶€ë¶„ ê°•ì¡°
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        attn_out = self.norm1(lstm_out + attn_out)
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì‚¬ìš©
        final_out = attn_out[:, -1, :]
        
        return self.classifier(final_out)

# ğŸ”¥ Peak ì˜ì—­ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì»¤ìŠ¤í…€ Loss
class PeakAwareLoss(nn.Module):
    def __init__(self, peak_weight=3.0, shape_weight=2.0):
        super().__init__()
        self.peak_weight = peak_weight
        self.shape_weight = shape_weight
        self.mse = nn.MSELoss(reduction='none')
        self.bce = nn.BCEWithLogitsLoss(reduction='none')
        
    def forward(self, pred_logits, target):
        pred_prob = torch.sigmoid(pred_logits.squeeze())
        target = target.squeeze()
        
        # ê¸°ë³¸ BCE Loss
        bce_loss = self.bce(pred_logits.squeeze(), target)
        
        # MSE Loss (shape ì •í™•ë„)
        mse_loss = self.mse(pred_prob, target)
        
        # Peak ì˜ì—­ ê°€ì¤‘ì¹˜ (target > 0.7ì¸ ì˜ì—­)
        peak_mask = target > 0.7
        if peak_mask.any():
            peak_bce = bce_loss[peak_mask].mean()
            peak_mse = mse_loss[peak_mask].mean()
        else:
            peak_bce = torch.tensor(0.0, device=pred_logits.device)
            peak_mse = torch.tensor(0.0, device=pred_logits.device)
        
        # ì „ì²´ loss
        total_bce = bce_loss.mean()
        total_mse = mse_loss.mean()
        
        final_loss = (total_bce + 
                     self.peak_weight * peak_bce + 
                     self.shape_weight * (total_mse + peak_mse))
        
        return final_loss

# ğŸ”¥ í›„ì²˜ë¦¬ë¡œ sharp peak ê°•ì œ ìƒì„±
class PredictionRefiner:
    def __init__(self, smoothing_window=20, peak_threshold=0.3):
        self.smoothing_window = smoothing_window
        self.peak_threshold = peak_threshold
    
    def refine(self, predictions):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì •ì œí•˜ì—¬ sharpí•œ peak ìƒì„±"""
        predictions = np.array(predictions)
        
        # 1. ë…¸ì´ì¦ˆ ì œê±° (ì´ë™í‰ê· )
        smoothed = np.convolve(predictions, 
                              np.ones(self.smoothing_window)/self.smoothing_window, 
                              mode='same')
        
        # 2. Peak ê°ì§€
        peaks, properties = find_peaks(smoothed, height=self.peak_threshold, distance=100)
        
        # 3. ê°€ì¥ ë†’ì€ peakë§Œ ì„ íƒ
        if len(peaks) > 0:
            best_peak_idx = peaks[np.argmax(smoothed[peaks])]
            
            # 4. ìƒˆë¡œìš´ sharp peak ìƒì„±
            refined = np.zeros_like(predictions)
            peak_width = 50
            
            for i in range(max(0, best_peak_idx - peak_width), 
                          min(len(refined), best_peak_idx + peak_width)):
                distance = abs(i - best_peak_idx)
                intensity = np.exp(-0.5 * (distance / (peak_width/3)) ** 2)
                refined[i] = intensity
                
            return refined
        
        return predictions

# Dataset í´ë˜ìŠ¤ë“¤
class train_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0800', '1000']
        self.seq_len = seq_len
        self.range_list = [i for i in range(1, 18)]  # train ë²”ìœ„ ì¡°ì •
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(
            self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]

class val_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0800', '1000']
        self.seq_len = seq_len
        self.range_list = [i for i in range(18, 21)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(
            self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]

# ğŸ”¥ ê°œì„ ëœ í•™ìŠµ í•¨ìˆ˜
def train_precise_model():
    # ë°ì´í„° ì¤€ë¹„ - í™•ì‹¤í•œ ê²½ë¡œ ì„¤ì •
    file_dir = ''  # ì‹¤ì œ ë°ì´í„° ê²½ë¡œ
    seq_len = 512
    batch_size = 256  # 128 â†’ 256ìœ¼ë¡œ ë³€ê²½
    
    train_ds = train_bin_dataset(file_dir, seq_len)
    val_ds = val_bin_dataset(file_dir, seq_len)
    
    print(f'Train: {len(train_ds)}, Val: {len(val_ds)}')
    
    # ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§
    binary_labels = [1 if y.item() > 0.5 else 0 for _, y in train_ds]
    class_counts = [binary_labels.count(0), binary_labels.count(1)]
    class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]
    weights = [class_weights[label] for label in binary_labels]
    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)
    
    train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)
    
    # ğŸ”¥ ê°œì„ ëœ ëª¨ë¸ ì‚¬ìš©
    model = ImprovedLSTMDetector(input_size=1, hidden_size=256, num_layers=2, output_size=1)
    model = nn.DataParallel(model).to(device)
    
    # ğŸ”¥ ê°œì„ ëœ í•™ìŠµ ì„¤ì • - lr ì¦ê°€, patience ì¦ê°€
    optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)  # 0.00001 â†’ 0.00005
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.7)  # verbose ì œê±°
    criterion = PeakAwareLoss(peak_weight=3.0, shape_weight=2.0)  # Peak ê°€ì¤‘ì¹˜ ë¶€ì—¬
    
    # í›„ì²˜ë¦¬ ê°ì²´
    refiner = PredictionRefiner()
    
    # í•™ìŠµ ì„¤ì •
    num_epochs = 50
    patience = 10  # 5 â†’ 10ìœ¼ë¡œ ì¦ê°€
    best_val_loss = float('inf')
    early_stop_counter = 0
    min_delta = 0.0002
    
    train_losses = []
    val_losses = []
    val_aucs = []
    
    # validation ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ (ì‹œê°í™”ë¥¼ ìœ„í•´)
    val_losses_history = []
    val_aucs_history = []
    val_preds_history = []   # ê° epochë³„ ì „ì²´ ì˜ˆì¸¡ê°’
    val_labels_history = []  # ê° epochë³„ ì „ì²´ ì •ë‹µ
    
    for epoch in range(num_epochs):
        print(f"\nğŸš€ Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
        model.train()
        epoch_loss = 0

        for batch, (X, y) in enumerate(train_dl):
            optimizer.zero_grad()
            X, y = X.to(device).unsqueeze(-1), y.to(device).float().squeeze()
            pred = model(X).squeeze()
            loss = criterion(pred, y)  # ğŸ¯ BCEWithLogitsLoss ì‚¬ìš©
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            train_loss = epoch_loss / len(train_dl)
        print("train loss: ", train_loss)

        model.eval()
        val_epoch_loss = 0
        # all_x_data = []
        all_preds = []
        # all_val_preds = []
        all_labels = []

        with torch.no_grad():
            for batch, (X, y) in enumerate(val_dl):
                X, val_labels = X.to(device).unsqueeze(-1), y.to(device).float()
                val_preds = model(X).squeeze()  # raw logits
                loss = criterion(val_preds, val_labels)
                val_epoch_loss += loss.item()
                probas = torch.sigmoid(val_preds)  # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
                X_data = X.cpu().numpy()
                # all_val_preds.extend(val_preds.cpu().numpy())
                # all_x_data.extend(X_data[-1])
                all_preds.extend(probas.cpu().numpy())
                all_labels.extend(y.cpu().numpy())

        val_loss = val_epoch_loss / len(val_dl)
        binary_labels = [1 if label > 0.8 else 0 for label in all_labels]
        val_auc = roc_auc_score(binary_labels, all_preds)

        print(f'ğŸ”¹ Validation Loss: {val_loss:.10f}, ROC-AUC Score: {val_auc:.5f}')

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_aucs.append(val_auc)

        # validation ê²°ê³¼ ì €ì¥ (ì‹œê°í™” ìš©)
        val_losses_history.append(val_loss)
        val_aucs_history.append(val_auc)
        val_preds_history.append(val_preds)
        val_labels_history.append(val_labels)

        # ğŸ¯ ì¡°ê¸° ì¢…ë£Œ ì²´í¬: val_loss ê¸°ì¤€
        if val_loss < (best_val_loss - min_delta):
            best_val_loss = val_loss
            early_stop_counter = 0
            torch.save(model.state_dict(), "./model/best_model_with_newdata_tuning.pth")  # ğŸ¯ ëª¨ë¸ ì €ì¥
            print(f"âœ… Model saved! New best val_loss: {best_val_loss:.5f}")
        else:
            early_stop_counter += 1
            print(f"âš ï¸ Early stopping counter: {early_stop_counter}/{patience}")

        if early_stop_counter >= patience:
            print("â¹ Early stopping activated. Stopping training.")
            break  # ğŸ¯ í•™ìŠµ ì¤‘ë‹¨

    print('ğŸ¯ Training Completed! Best Validation Loss:', best_val_loss)
    return model, refiner

# ì‹¤í–‰
if __name__ == "__main__":
    model, refiner = train_precise_model()
