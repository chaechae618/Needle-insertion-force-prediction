import os
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
import torch.nn.functional as F

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"  # Arrange GPU devices starting from 0
os.environ["CUDA_VISIBLE_DEVICES"]= "0, 1, 2"  # Set the GPUs 2 and 3 to use
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('Device:', device)
print('Current cuda device:', torch.cuda.current_device())
print('Count of using GPUs:', torch.cuda.device_count())
torch.cuda.manual_seed_all(42)

def gaussian_kernel1d(kernel_size, sigma):
    # kernel_size는 홀수여야 함
    x = torch.arange(kernel_size, dtype=torch.float32) - (kernel_size - 1) / 2.0
    kernel = torch.exp(-0.5 * (x / sigma) ** 2)
    kernel = kernel / kernel.sum()  # 합이 1이 되도록 정규화
    return kernel


def smooth_label_signal(signal, kernel_size=11, sigma=2.0):
    kernel = gaussian_kernel1d(kernel_size, sigma).to(signal.device)
    kernel = kernel.view(1, 1, kernel_size)  # conv1d용 shape
    padding = kernel_size // 2  # 입력과 출력 길이 동일하게 유지
    smoothed_signal = F.conv1d(signal, kernel, padding=padding)
    return smoothed_signal

def replace_zeros_with_random_torch(tensor, low=0.0, high=0.1, seed=42):
    torch.manual_seed(seed)
    mask = tensor <= 0.1
    # mask가 True인 위치에 대해 랜덤 값을 생성해서 대입
    random_vals = torch.empty(mask.sum(), device=tensor.device).uniform_(low, high)
    tensor[mask] = random_vals
    return tensor

def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []
    motor_x_seq = []
    needle_dis = ['0800', '1000']
    file_dir = '/home/ibom002/dataset/Data_20250709'

    for ind in needle_dis:
        for i in range_list:
            file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            with open(file_path, 'rb') as file1:
                data = np.frombuffer(file1.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                nor_x =  data[0] - data[0][0]
                indices = np.where(data[4] == 1)[0]
                array_size = len(data[4])
                label_array = np.full(array_size, 0, dtype=np.float32)
                label_array[indices[1]-20 : indices[1]+20] = 1
                
                if ind == '0600':
                    try:
                        # 임계값을 초과하는 데이터의 인덱스 찾기
                        indices_high = np.where(data[0] > 3.05)[0]
                        indices_low = np.where(data[0] < -3.05)[0]
                        # 100 이상 차이나는 지점 찾기
                        idx_diff_low = np.where(np.diff(indices_low) > 100)[0]

                        # 삭제할 구간 설정
                        start_1 = indices_high[0]  # 시작 구간 1
                        end_1 = indices_low[idx_diff_low[0]]  # 끝 구간 1
                        start_2 = indices_low[idx_diff_low[0] + 1]  # 시작 구간 2
                        end_2 = indices_high[-1]  # 끝 구간 2

                        # start_1 ~ end_2 구간에 6.1을 더하기
                        data[0][start_1:end_2 + 1] += 6.1  # 해당 구간에 6.1 추가

                        # 필터링을 위한 마스크 생성
                        indices = np.arange(len(data[0]))  # 전체 인덱스 생성
                        mask = ~((indices >= start_1) & (indices <= end_1) | (indices >= start_2) & (indices <= end_2))

                        # 특정 구간 제거
                        x_data = data[0][mask]
                        nor_x =  x_data - data[0][0]
                        label_array = label_array[mask]
                    except Exception as e:
                        print(f"⚠️ 예외 발생 (ind: {ind}, i: {i}) - {e}")
                        pass  # 예외가 발생해도 무시하고 계속 진행
                        
                        
                smoothed_signal = smooth_label_signal(torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0), kernel_size=10001, sigma=500.0)
                smoothed_signal = smoothed_signal / smoothed_signal.max()  # 최대값을 1로 재정규화
                smoothed_signal = smoothed_signal.squeeze(0).squeeze(0)
                smoothed_signal = replace_zeros_with_random_torch(smoothed_signal)
                
                for j in range(seq_len, len(nor_x) - seq_len):
                    x_win = nor_x[j - seq_len : j]
                    x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                    
                    y_win = smoothed_signal[j]
                    y_seq.append(torch.tensor(y_win, dtype=torch.float64))
                print(f'{ind} _ {i} file processed. Data length: {len(nor_x)}, Labels: {len(label_array)}')
                # motor_x_seq.append(data[2]) # motor input
                            
    
    print(f'total sequence : {len(x_seq)}')
    return x_seq, y_seq

class train_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0600','0800', '1000']
        self.seq_len = seq_len
        self.x_seq = []
        self.y_seq = []
        self.range_list = [i for i in range(1, 19)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]               
    
class val_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0600', '0800', '1000']
        self.seq_len = seq_len
        self.x_seq = []
        self.y_seq = []
        self.range_list = [i for i in range(18, 21)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]                 

file_dir = '/home/ibom002/dataset/Data_20250709'
seq_len = 512
batch_size = 512

train_ds = train_bin_dataset(file_dir, seq_len)
val_ds = val_bin_dataset(file_dir, seq_len)

print('train_ds_len', len(train_ds))
print('val_ds_len', len(val_ds))

# 이진화: 0.5를 임계값으로 사용 (필요에 따라 조정)
binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
class_counts = [binary_labels.count(0), binary_labels.count(1)]
class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]  # 반비례 가중치

# 샘플 가중치 설정: 이진화한 레이블을 기준으로 가중치 부여
weights = [class_weights[label] for label in binary_labels]
sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

# 데이터로더에 샘플러 추가
train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, shuffle=False, drop_last=True)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)

print('train_dl_len', len(train_dl))
print('val_dl_len', len(val_dl))

# ✅ LSTM 모델 (input_size=1 수정)
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMAnomalyDetector, self).__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size,512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]  # 마지막 타임스텝의 출력
        fc1_out = self.fc1(lstm_out)
        fc2_out = self.fc2(fc1_out)
        return self.fc3(fc2_out)  # 🎯 BCEWithLogitsLoss를 쓰므로 sigmoid 적용 X
# ✅ train 함수 (중간 `val()` 실행 추가)
def train(train_dl, val_dl, model, criterion, optimizer, epoch, eval_every=1000):
    model.train()
    epoch_loss = 0

    for batch, (X, y) in enumerate(train_dl):
        optimizer.zero_grad()
        X, y = X.to(device).unsqueeze(-1), y.to(device).float().squeeze()
        pred = model(X).squeeze()
        loss = criterion(pred, y)  # 🎯 BCEWithLogitsLoss 사용
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        
        # 🎯 중간 평가 실행 (매 `eval_every` 배치마다 `val()` 실행)
        if batch % eval_every == 0 and batch != 0:
            val_loss, val_auc, val_preds, val_labels = val(val_dl, model, criterion)
            print(f'🔹 Epoch {epoch+1} | Batch {batch}: Validation AUC = {val_auc:.5f} | Validation loos = {val_loss:.5f}')
            model.train()
    return epoch_loss / len(train_dl)

# ✅ val 함수 (ROC-AUC 계산 추가)
def val(val_dl, model, criterion):
    model.eval()
    epoch_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch, (X, y) in enumerate(val_dl):
            X, y = X.to(device).unsqueeze(-1), y.to(device).float()
            pred = model(X).squeeze()  # raw logits
            loss = criterion(pred, y)
            epoch_loss += loss.item()
            probas = torch.sigmoid(pred)  # 확률값으로 변환
            all_preds.extend(probas.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    avg_loss = epoch_loss / len(val_dl)
    auc_score = roc_auc_score(all_labels, all_preds)
    print(f'🔹 Validation Loss: {avg_loss:.10f}, ROC-AUC Score: {auc_score:.5f}')
    return avg_loss, auc_score, all_preds, all_labels

#1. lstm + early stopping

# ✅ 모델 설정 옵티마이저 & 손실 함수
model = LSTMAnomalyDetector(input_size=1, hidden_size=512, num_layers=2, output_size=1)
model = nn.DataParallel(model).to(device)

optimizer = optim.Adam(model.parameters(), lr=0.00001)
criterion = nn.BCEWithLogitsLoss()

# ✅ 조기 종료(Early Stopping) 설정
patience = 5         # 개선되지 않는 Epoch 수 (조기 종료 기준)
best_val_loss = float('inf')  # 현재까지 가장 낮은 Validation Loss
early_stop_counter = 0        # 개선되지 않은 Epoch 수 카운트
min_delta = 0.0002            # Loss 개선 최소 기준 (너무 미세한 차이는 무시)


# ✅ 학습 실행 (조기 종료 포함)
num_epochs = 100
train_losses = []
val_losses = []
val_aucs = []

# validation 결과를 저장하기 위한 리스트 (시각화를 위해)
val_losses_history = []
val_aucs_history = []
val_preds_history = []   # 각 epoch별 전체 예측값
val_labels_history = []  # 각 epoch별 전체 정답

for epoch in range(num_epochs):
    print(f"\n🚀 Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
    model.train()
    epoch_loss = 0

    for batch, (X, y) in enumerate(train_dl):
        optimizer.zero_grad()
        X, y = X.to(device).unsqueeze(-1), y.to(device).float().squeeze()
        pred = model(X).squeeze()
        loss = criterion(pred, y)  # 🎯 BCEWithLogitsLoss 사용
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        train_loss = epoch_loss / len(train_dl)
    print("train loss: ", train_loss)
    
    model.eval()
    val_epoch_loss = 0
    # all_x_data = []
    all_preds = []
    # all_val_preds = []
    all_labels = []
    
    
    with torch.no_grad():
        for batch, (X, y) in enumerate(val_dl):
            X, val_labels = X.to(device).unsqueeze(-1), y.to(device).float()
            val_preds = model(X).squeeze()  # raw logits
            loss = criterion(val_preds, val_labels)
            val_epoch_loss += loss.item()
            probas = torch.sigmoid(val_preds)  # 확률값으로 변환
            X_data = X.cpu().numpy()
            # all_val_preds.extend(val_preds.cpu().numpy())
            # all_x_data.extend(X_data[-1])
            all_preds.extend(probas.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    val_loss = val_epoch_loss / len(val_dl)
    binary_labels = [1 if label > 0.8 else 0 for label in all_labels]
    val_auc = roc_auc_score(binary_labels, all_preds)
    
    print(f'🔹 Validation Loss: {val_loss:.10f}, ROC-AUC Score: {val_auc:.5f}')
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_aucs.append(val_auc)
    
    # validation 결과 저장 (시각화 용)
    val_losses_history.append(val_loss)
    val_aucs_history.append(val_auc)
    val_preds_history.append(val_preds)
    val_labels_history.append(val_labels)
 
    # 🎯 조기 종료 체크: val_loss 기준
    if val_loss < (best_val_loss - min_delta):
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), "./model/best_model.pth")  # 🎯 모델 저장
        print(f"✅ Model saved! New best val_loss: {best_val_loss:.5f}")
    else:
        early_stop_counter += 1
        print(f"⚠️ Early stopping counter: {early_stop_counter}/{patience}")

    if early_stop_counter >= patience:
        print("⏹ Early stopping activated. Stopping training.")
        break  # 🎯 학습 중단

print('🎯 Training Completed! Best Validation Loss:', best_val_loss)

# 2. kernel size = 2001, sigma 200 -> 시간퍼짐 감소, 슬라이싱 윈도우 정렬 (예측시점중심)
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ✅ Smoothed label 생성 함수 (가정: 이미 정의돼 있음)
def smooth_label_signal(x, kernel_size=2001, sigma=200.0):
    import torch.nn.functional as F
    import math
    half = kernel_size // 2
    x = x.float()

    # 1D Gaussian 커널
    t = torch.arange(-half, half + 1, dtype=torch.float32)
    gauss = torch.exp(-t ** 2 / (2 * sigma ** 2))
    gauss /= gauss.sum()

    gauss = gauss.to(x.device)
    gauss = gauss.view(1, 1, -1)

    # padding 후 convolution
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss)


# ✅ 데이터 전처리 함수
def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []

    for ind in needle_dis:
        for i in range(range_list[0], range_list[1] + 1):
            file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            try:
                with open(file_path, 'rb') as f:
                    data = np.frombuffer(f.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                    force = data[0]
                    norm_force = force - force[0]
                    label_raw = data[4]

                    indices = np.where(label_raw == 1)[0]
                    if len(indices) < 2:
                        continue
                    second_puncture = indices[1]

                    label_array = np.zeros_like(label_raw)
                    label_array[second_puncture - 20 : second_puncture + 20] = 1

                    smoothed = smooth_label_signal(
                        torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0),
                        kernel_size=2001, sigma=200.0
                    ).squeeze().numpy()
                    smoothed = smoothed / smoothed.max()

                    for j in range(0, len(norm_force) - seq_len):
                        x_win = norm_force[j : j + seq_len]
                        y_idx = j + seq_len // 2
                        if y_idx >= len(smoothed):
                            break
                        y_val = smoothed[y_idx]
                        x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                        y_seq.append(torch.tensor(y_val, dtype=torch.float32))
            except Exception as e:
                print(f"⚠️ Error loading {file_path}: {e}")
    return x_seq, y_seq


# ✅ Dataset 클래스
class bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len, mode='train'):
        self.needle_dis = [ '0800', '1000']
        self.seq_len = seq_len
        self.file_dir = file_dir
        if mode == 'train':
            self.range_list = (1, 17)
        else:
            self.range_list = (18, 20)
        self.x_seq, self.y_seq = Data_preprocessing(
            self.needle_dis, self.seq_len, self.file_dir, self.range_list
        )

    def __len__(self):
        return len(self.y_seq)

    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]


# ✅ 모델 구조
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        return self.fc3(self.fc2(self.fc1(lstm_out)))


# ✅ 데이터 준비
file_dir = '/home/ibom002/dataset/Data_20250709'
seq_len = 512
batch_size = 128

train_ds = bin_dataset(file_dir, seq_len, mode='train')
val_ds = bin_dataset(file_dir, seq_len, mode='val')

# ✅ 가중치 기반 샘플링
binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
class_counts = [binary_labels.count(0), binary_labels.count(1)]
class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]
weights = [class_weights[label] for label in binary_labels]
sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)


# ✅ 모델 & 학습 설정
model = LSTMAnomalyDetector(input_size=1, hidden_size=512, num_layers=2, output_size=1)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-5)
criterion = nn.BCEWithLogitsLoss()

# ✅ 학습 루프
num_epochs = 100
best_val_loss = float('inf')
early_stop_counter = 0
patience = 5
min_delta = 0.0002

for epoch in range(num_epochs):
    print(f"\n🚀 Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
    model.train()
    train_loss = 0
    for X, y in train_dl:
        X, y = X.to(device).unsqueeze(-1), y.to(device)
        pred = model(X).squeeze()
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_dl)
    print("Train Loss:", train_loss)

    model.eval()
    val_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X, y in val_dl:
            X, y = X.to(device).unsqueeze(-1), y.to(device)
            pred = model(X).squeeze()
            loss = criterion(pred, y)
            val_loss += loss.item()
            all_preds.extend(torch.sigmoid(pred).cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    val_loss /= len(val_dl)
    val_auc = roc_auc_score([1 if l > 0.8 else 0 for l in all_labels], all_preds)
    print(f"🔹 Validation Loss: {val_loss:.6f}, ROC-AUC: {val_auc:.5f}")

    # ✅ 조기 종료
    if val_loss < best_val_loss - min_delta:
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), './model/best_model.pth')
        print(f"✅ Saved new best model with val_loss: {val_loss:.5f}")
    else:
        early_stop_counter += 1
        print(f"⚠️ Early stopping counter: {early_stop_counter}/{patience}")

    if early_stop_counter >= patience:
        print("⏹ Early stopping triggered.")
        break

print(f"\n🎯 Training finished. Best Validation Loss: {best_val_loss:.5f}")

# 3. 첫번째 puncture 무시, 두번째 puncture 중심학습, smoothing 범위 감소, 중심예측구조
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

# ✅ 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ✅ Gaussian smoothing 함수 (정밀 스무딩)
def smooth_label_signal(x, kernel_size=101, sigma=10.0):
    import torch.nn.functional as F
    half = kernel_size // 2
    x = x.float()
    t = torch.arange(-half, half + 1, dtype=torch.float32)
    gauss = torch.exp(-t ** 2 / (2 * sigma ** 2))
    gauss /= gauss.sum()
    gauss = gauss.to(x.device).view(1, 1, -1)
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss)

# ✅ 데이터 전처리 함수
def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []

    for ind in needle_dis:
        for i in range(range_list[0], range_list[1] + 1):
            file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            try:
                with open(file_path, 'rb') as f:
                    data = np.frombuffer(f.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                    force = data[0]
                    norm_force = force - force[0]
                    label_raw = data[4]

                    indices = np.where(label_raw == 1)[0]
                    if len(indices) < 2:
                        continue
                    second_puncture = indices[1]

                    label_array = np.zeros_like(label_raw)
                    label_array[second_puncture] = 1  # 정확한 위치만 1

                    smoothed = smooth_label_signal(
                        torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0),
                        kernel_size=101, sigma=10.0
                    ).squeeze().numpy()
                    smoothed = smoothed / smoothed.max()

                    for j in range(0, len(norm_force) - seq_len):
                        x_win = norm_force[j : j + seq_len]
                        y_idx = j + seq_len // 2
                        if y_idx >= len(smoothed):
                            break
                        y_val = smoothed[y_idx]
                        x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                        y_seq.append(torch.tensor(y_val, dtype=torch.float32))
            except Exception as e:
                print(f"⚠️ Error loading {file_path}: {e}")
    return x_seq, y_seq

# ✅ Dataset 클래스
class bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len, mode='train'):
        self.needle_dis = ['0800', '1000']
        self.seq_len = seq_len
        self.file_dir = file_dir
        self.range_list = (1, 17) if mode == 'train' else (18, 20)
        self.x_seq, self.y_seq = Data_preprocessing(
            self.needle_dis, self.seq_len, self.file_dir, self.range_list
        )

    def __len__(self):
        return len(self.y_seq)

    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]

# ✅ 모델 정의
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        return self.fc3(self.fc2(self.fc1(lstm_out)))

# ✅ 학습 데이터 준비
file_dir = '/home/ibom002/dataset/Data_20250709'
seq_len = 512
batch_size = 128

train_ds = bin_dataset(file_dir, seq_len, mode='train')
val_ds = bin_dataset(file_dir, seq_len, mode='val')

# ✅ 가중치 샘플링
binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
class_counts = [binary_labels.count(0), binary_labels.count(1)]
class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]
weights = [class_weights[label] for label in binary_labels]
sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)

# ✅ 모델 학습 설정
model = LSTMAnomalyDetector(input_size=1, hidden_size=512, num_layers=2, output_size=1).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-5)
criterion = nn.BCEWithLogitsLoss()

# ✅ 학습 루프
num_epochs = 50
best_val_loss = float('inf')
early_stop_counter = 0
patience = 7
min_delta = 0.0002

for epoch in range(num_epochs):
    print(f"\n🚀 Epoch {epoch+1}/{num_epochs}\n--------------------------------------------------------")
    model.train()
    train_loss = 0
    for X, y in train_dl:
        X, y = X.to(device).unsqueeze(-1), y.to(device)
        pred = model(X).squeeze()
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_dl)
    print("Train Loss:", train_loss)

    model.eval()
    val_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X, y in val_dl:
            X, y = X.to(device).unsqueeze(-1), y.to(device)
            pred = model(X).squeeze()
            loss = criterion(pred, y)
            val_loss += loss.item()
            all_preds.extend(torch.sigmoid(pred).cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    val_loss /= len(val_dl)
    val_auc = roc_auc_score([1 if l > 0.8 else 0 for l in all_labels], all_preds)
    print(f"🔹 Validation Loss: {val_loss:.6f}, ROC-AUC: {val_auc:.5f}")

    if val_loss < best_val_loss - min_delta:
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), './model/best_model.pth')
        print(f"✅ Saved new best model with val_loss: {val_loss:.5f}")
    else:
        early_stop_counter += 1
        print(f"⚠️ Early stopping counter: {early_stop_counter}/{patience}")

    if early_stop_counter >= patience:
        print("⏹ Early stopping triggered.")
        break

print(f"\n🎯 Training finished. Best Validation Loss: {best_val_loss:.5f}")

# 4. 
# False-positive Loss – y<0.1 구간에서 sigmoid(pred)>0.5인 샘플에 추가 페널티를 주면, 모델이 “여기는 절대 puncture가 아니다”라는 신호를 더 강하게 학습합니다.
# – 학습이 진행될수록 앞부분에 높은 출력이 나오지 않도록 weight가 조정됩니다.
# Weighted Sampler – negative/positive 비율을 균등하게 뽑아 주므로, 전체가 negative인 구간에서의 학습 비중이 올라가 false-positive 확률이 자연스럽게 감소합니다.
# Asymmetric Label Smoothing – 라벨 스무딩 커널을 앞쪽(과거)에 크게, 뒤쪽(미래)에 작게 주면 ‘진짜 puncture 직후’에만 라벨 효과가 집중되고, 그 이전 구간엔 거의 영향을 주지 않습니다.
# Post-processing (smoothing + peak filter)– 검증 시 gaussian_filter1d로 부드럽게 한 뒤 find_peaks(height=0.5, distance=100)를 쓰면, 첫 번째(잘못된) 작은 bump는 높이 기준(0.5)이나 최소 거리(100 스텝) 조건을 충족하지 못해 필터링되고, 두 번째(진짜) 큰 peak만 검출됩니다.
# 얼리스타핑 5 -> 15

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

# post-processing 용
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks

# ✅ 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ✅ Asymmetric Gaussian smoothing 함수 (비대칭 라벨 스무딩)
def smooth_label_signal(x, left_sigma=300.0, right_sigma=100.0, kernel_size=1001):
    import torch.nn.functional as F
    half = kernel_size // 2
    t = torch.linspace(-1, 1, steps=kernel_size, device=x.device)
    left_mask = t < 0
    right_mask = ~left_mask
    left = torch.exp(-0.5 * (t[left_mask]**2) / (left_sigma**2))
    right = torch.exp(-0.5 * (t[right_mask]**2) / (right_sigma**2))
    gauss = torch.cat([left, right])
    gauss /= gauss.sum()
    gauss = gauss.view(1, 1, -1)

    x = x.float().unsqueeze(0).unsqueeze(0)  # [1,1,T]
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss).squeeze()

# ✅ 데이터 전처리 함수 (개선됨)
def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq, y_seq, time_seq = [], [], []  # time_seq 추가
    for ind in needle_dis:
        for i in range(range_list[0], range_list[1] + 1):
            fp = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
            try:
                data = np.fromfile(fp, dtype=np.float32).reshape(-1,5).T.copy()
                force = data[0]
                norm_force = force - force[0]
                raw_lbl = data[4]
                idxs = np.where(raw_lbl==1)[0]
                if len(idxs)<2: continue
                second = idxs[1]
                lbl_arr = np.zeros_like(raw_lbl, dtype=np.float32)
                lbl_arr[second] = 1.0
                sm = smooth_label_signal(torch.from_numpy(lbl_arr), left_sigma=300.0, right_sigma=100.0, kernel_size=1001).numpy()
                sm /= sm.max()
                
                # 시간 정보 추가
                for j in range(len(norm_force)-seq_len):
                    x_seq.append(torch.tensor(norm_force[j:j+seq_len], dtype=torch.float32))
                    y_seq.append(torch.tensor(sm[j+seq_len//2], dtype=torch.float32))
                    time_seq.append(j+seq_len//2)  # 중간 시점의 시간 인덱스
            except Exception as e:
                print(f"⚠️ Error loading {fp}: {e}")
    return x_seq, y_seq, time_seq

# ✅ Dataset 클래스 (개선됨)
class bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len, mode='train'):
        self.needle_dis = ['0800','1000']
        self.seq_len = seq_len
        self.file_dir = file_dir
        self.range_list = (1,17) if mode=='train' else (18,20)
        self.x_seq, self.y_seq, self.time_seq = Data_preprocessing(self.needle_dis, self.seq_len, self.file_dir, self.range_list)
    def __len__(self): return len(self.y_seq)
    def __getitem__(self, idx): 
        return self.x_seq[idx], self.y_seq[idx], self.time_seq[idx]

# ✅ 개선된 모델 정의 (Attention 메커니즘 추가)
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)
        
        # Time-aware feature
        self.time_embedding = nn.Linear(1, hidden_size // 4)
        
        # Classification layers
        self.fc1 = nn.Linear(hidden_size + hidden_size // 4, 512)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(256, output_size)
        
    def forward(self, x, time_step=None):
        # LSTM
        h, _ = self.lstm(x)
        
        # Attention (최근 시점에 더 집중)
        attn_out, _ = self.attention(h, h, h)
        
        # 마지막 출력
        h_last = attn_out[:, -1, :]
        
        # Time embedding (시간 정보 추가)
        if time_step is not None:
            time_emb = self.time_embedding(time_step.unsqueeze(-1).float())
            h_last = torch.cat([h_last, time_emb], dim=1)
        
        # Classification
        h = torch.relu(self.fc1(h_last))
        h = self.dropout1(h)
        h = torch.relu(self.fc2(h))
        h = self.dropout2(h)
        return self.fc3(h)

# ✅ 개선된 손실 함수 (시간 기반 페널티)
class TimePenaltyLoss(nn.Module):
    def __init__(self, base_criterion, early_penalty_weight=2.0, early_threshold=5000):
        super().__init__()
        self.base_criterion = base_criterion
        self.early_penalty_weight = early_penalty_weight
        self.early_threshold = early_threshold
    
    def forward(self, pred, target, time_step):
        # 기본 손실
        base_loss = self.base_criterion(pred, target)
        
        # 초기 시점에서 높은 예측에 대한 페널티
        early_mask = time_step < self.early_threshold
        high_pred_mask = torch.sigmoid(pred) > 0.5
        early_fp_mask = early_mask & high_pred_mask & (target < 0.1)
        
        if early_fp_mask.any():
            early_penalty = self.early_penalty_weight * self.base_criterion(pred[early_fp_mask], target[early_fp_mask])
            base_loss += early_penalty
        
        return base_loss

# ✅ 하이퍼파라미터
file_dir   = '/home/ibom002/dataset/Data_20250709'
seq_len    = 512
batch_size = 128
num_epochs = 50

# ✅ 데이터 로드 & Sampler
train_ds = bin_dataset(file_dir, seq_len, mode='train')
val_ds   = bin_dataset(file_dir, seq_len, mode='val')
labels   = [1 if y.item()>0.8 else 0 for _,y,_ in train_ds]
counts   = [labels.count(0), labels.count(1)]
weights  = [1.0/counts[l] for l in labels]
sampler  = WeightedRandomSampler(weights, len(weights), replacement=True)
train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_dl   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)

# ✅ 모델·옵티마이저·손실
model     = LSTMAnomalyDetector(1,512,2,1).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)
criterion = TimePenaltyLoss(nn.BCEWithLogitsLoss(), early_penalty_weight=3.0, early_threshold=5000)

# ✅ Early-stopping 세팅
best_metric       = 0.0
patience          = 15
min_delta         = 0.001
early_stop_counter= 0

for epoch in range(1, num_epochs+1):
    print(f"\n🚀 Epoch {epoch}/{num_epochs}")
    # --- Train ---
    model.train()
    train_loss = 0
    for X, y, time_step in train_dl:
        X, y, time_step = X.unsqueeze(-1).to(device), y.to(device), time_step.to(device)
        pred = model(X, time_step).squeeze()
        loss = criterion(pred, y, time_step)
        
        # 추가적인 False Positive 억제
        fp_mask = (y<0.1)&(torch.sigmoid(pred)>0.5)
        if fp_mask.any():
            loss += 0.5*nn.BCEWithLogitsLoss()(pred[fp_mask],y[fp_mask])
        
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_dl)
    print(f"  Train Loss: {train_loss:.6f}")

    # --- Validation ---
    model.eval()
    val_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X, y, time_step in val_dl:
            X, y, time_step = X.unsqueeze(-1).to(device), y.to(device), time_step.to(device)
            pred = model(X, time_step).squeeze()
            val_loss += nn.BCEWithLogitsLoss()(pred,y).item()
            all_preds.extend(torch.sigmoid(pred).cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    val_loss /= len(val_dl)

    # Post‐processing + 지표
    raw = np.array(all_preds)
    smth= gaussian_filter1d(raw, sigma=10)
    smth_auc = roc_auc_score([1 if l>0.8 else 0 for l in all_labels], smth)
    print(f"  Val Loss: {val_loss:.6f}, SMTH ROC-AUC: {smth_auc:.5f}")

    # --- Early Stopping (SMTH ROC-AUC 기준) ---
    if smth_auc > best_metric + min_delta:
        best_metric = smth_auc
        early_stop_counter = 0
        torch.save(model.state_dict(), './model/best2_model.pth')
        print(f"  ✅ Improved SMTH ROC-AUC to {smth_auc:.5f}, saved model.")
    else:
        early_stop_counter += 1
        print(f"  ⚠️ No improvement ({early_stop_counter}/{patience})")
        if early_stop_counter >= patience:
            print("  ⏹ Early stopping triggered.")
            break

print(f"\n🎯 Training finished. Best SMTH ROC-AUC: {best_metric:.5f}")

# ✅ 추가: 시간별 예측 분석 함수
def analyze_predictions_by_time(model, val_dl, device):
    model.eval()
    time_preds = []
    
    with torch.no_grad():
        for X, y, time_step in val_dl:
            X, y, time_step = X.unsqueeze(-1).to(device), y.to(device), time_step.to(device)
            pred = model(X, time_step).squeeze()
            pred_prob = torch.sigmoid(pred)
            
            for i in range(len(time_step)):
                time_preds.append({
                    'time': time_step[i].item(),
                    'prediction': pred_prob[i].item(),
                    'label': y[i].item()
                })
    
    return time_preds

# 5.  puncture 근처만 focus
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks

# ✅ 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ✅ 정밀한 Asymmetric Gaussian smoothing
def smooth_label_signal(x, left_sigma=100.0, right_sigma=30.0, kernel_size=401):
    """매우 좁은 범위의 스무딩으로 정확한 puncture 시점 표현"""
    half = kernel_size // 2
    t = torch.linspace(-1, 1, steps=kernel_size, device=x.device)
    left_mask = t < 0
    right_mask = ~left_mask
    left = torch.exp(-0.5 * (t[left_mask]**2) / (left_sigma**2))
    right = torch.exp(-0.5 * (t[right_mask]**2) / (right_sigma**2))
    gauss = torch.cat([left, right])
    gauss /= gauss.sum()
    gauss = gauss.view(1, 1, -1)

    x = x.float().unsqueeze(0).unsqueeze(0)
    x_padded = F.pad(x, (half, half), mode='reflect')
    return F.conv1d(x_padded, gauss).squeeze()

# ✅ 기존 모델 정의 (원본 코드에서 가져온 것)
class LSTMAnomalyDetector(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)
        self.time_embedding = nn.Linear(1, hidden_size // 4)
        self.fc1 = nn.Linear(hidden_size + hidden_size // 4, 512)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(256, output_size)
        
    def forward(self, x, time_step=None):
        h, _ = self.lstm(x)
        attn_out, _ = self.attention(h, h, h)
        h_last = attn_out[:, -1, :]
        
        if time_step is not None:
            time_emb = self.time_embedding(time_step.unsqueeze(-1).float())
            h_last = torch.cat([h_last, time_emb], dim=1)
        
        h = torch.relu(self.fc1(h_last))
        h = self.dropout1(h)
        h = torch.relu(self.fc2(h))
        h = self.dropout2(h)
        return self.fc3(h)

# ✅ 향상된 Focused Dataset (여러 파일 지원)
class FocusedPunctureDataset(Dataset):
    def __init__(self, file_dir, needle_list=['0800', '1000'], file_ranges=None, seq_len=512, window=1500):
        """
        Args:
            file_dir: 데이터 디렉토리
            needle_list: 니들 타입 리스트
            file_ranges: 파일 범위 딕셔너리 {'train': (1,17), 'val': (18,20)}
            seq_len: 시퀀스 길이
            window: puncture 주변 윈도우 크기
        """
        self.x, self.y, self.t, self.file_info = [], [], [], []
        
        if file_ranges is None:
            file_ranges = {'train': (1, 17), 'val': (18, 20)}
        
        for needle in needle_list:
            for mode, (start, end) in file_ranges.items():
                for file_idx in range(start, end + 1):
                    self._load_file(file_dir, needle, file_idx, seq_len, window)
        
        print(f"📊 Focused Dataset loaded: {len(self.x)} samples")
        print(f"   High confidence samples: {sum(1 for y in self.y if y > 0.5)}")
    
    def _load_file(self, file_dir, needle, file_idx, seq_len, window):
        """단일 파일 로드"""
        fp = os.path.join(file_dir, f'T2D_{needle}', f'SavedData_{file_idx:03d}.bin')
        try:
            data = np.fromfile(fp, dtype=np.float32).reshape(-1, 5).T.copy()
            force = data[0]
            norm_force = force - force[0]
            raw_lbl = data[4]
            idxs = np.where(raw_lbl == 1)[0]
            
            if len(idxs) < 2:
                return
            
            second = idxs[1]  # 정확한 puncture 시점
            
            # 매우 정밀한 라벨 생성
            lbl_arr = np.zeros_like(raw_lbl, dtype=np.float32)
            lbl_arr[second] = 1.0
            sm = smooth_label_signal(torch.from_numpy(lbl_arr)).numpy()
            sm /= sm.max()
            
            # Puncture 근처 데이터만 추출
            start = max(0, second - window)
            end = min(len(force), second + window)
            
            # 오버랩핑 시퀀스 생성 (더 조밀하게)
            step_size = seq_len // 8  # 더 많은 오버랩
            for j in range(start, end - seq_len, step_size):
                center_idx = j + seq_len // 2
                self.x.append(torch.tensor(norm_force[j:j+seq_len], dtype=torch.float32))
                self.y.append(torch.tensor(sm[center_idx], dtype=torch.float32))
                self.t.append(center_idx)
                self.file_info.append((needle, file_idx, j))
                
        except Exception as e:
            print(f"⚠️ Error loading {fp}: {e}")
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.t[idx]

# ✅ 향상된 Sharp Puncture Loss
class SharpPunctureLoss(nn.Module):
    def __init__(self, base_criterion=nn.BCEWithLogitsLoss(), 
                 early_threshold=8000, late_threshold=14000, 
                 penalty_weight=5.0, focus_weight=10.0):
        super().__init__()
        self.base = base_criterion
        self.early_threshold = early_threshold
        self.late_threshold = late_threshold
        self.penalty_weight = penalty_weight
        self.focus_weight = focus_weight
    
    def forward(self, pred, target, time_step):
        base_loss = self.base(pred, target)
        pred_prob = torch.sigmoid(pred)
        
        # 1. 초기 시점 False Positive 억제
        early_mask = time_step < self.early_threshold
        early_fp = early_mask & (pred_prob > 0.3) & (target < 0.1)
        if early_fp.any():
            early_penalty = self.penalty_weight * self.base(pred[early_fp], target[early_fp])
            base_loss += early_penalty
        
        # 2. 후기 시점 False Positive 억제
        late_mask = time_step > self.late_threshold
        late_fp = late_mask & (pred_prob > 0.3) & (target < 0.1)
        if late_fp.any():
            late_penalty = self.penalty_weight * self.base(pred[late_fp], target[late_fp])
            base_loss += late_penalty
        
        # 3. 정확한 puncture 시점 집중 강화
        high_target = target > 0.5
        if high_target.any():
            focus_loss = self.focus_weight * F.mse_loss(pred_prob[high_target], target[high_target])
            base_loss += focus_loss
        
        # 4. 시간 기반 가중치
        time_weight = torch.ones_like(time_step)
        puncture_region = (time_step >= 11000) & (time_step <= 13000)
        time_weight[puncture_region] = 2.0
        
        weighted_loss = base_loss * time_weight.mean()
        return weighted_loss

# ✅ 평가 함수
def evaluate_focused_model(model, dataloader, device):
    model.eval()
    all_preds, all_labels, all_times = [], [], []
    
    with torch.no_grad():
        for X, y, t in dataloader:
            X, y, t = X.unsqueeze(-1).to(device), y.to(device), t.to(device)
            pred = model(X, t).squeeze()
            pred_prob = torch.sigmoid(pred)
            
            all_preds.extend(pred_prob.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
            all_times.extend(t.cpu().numpy())
    
    # 시간 순서로 정렬
    sorted_indices = np.argsort(all_times)
    sorted_preds = np.array(all_preds)[sorted_indices]
    sorted_labels = np.array(all_labels)[sorted_indices]
    sorted_times = np.array(all_times)[sorted_indices]
    
    # 스무딩 후 피크 검출
    smoothed_preds = gaussian_filter1d(sorted_preds, sigma=3)
    peaks, properties = find_peaks(smoothed_preds, height=0.3, distance=50, prominence=0.1)
    
    # 가장 높은 피크 찾기
    if len(peaks) > 0:
        peak_heights = smoothed_preds[peaks]
        max_peak_idx = peaks[np.argmax(peak_heights)]
        predicted_time = sorted_times[max_peak_idx]
        confidence = peak_heights[np.argmax(peak_heights)]
    else:
        predicted_time = None
        confidence = 0.0
    
    # 성능 지표
    binary_labels = (np.array(all_labels) > 0.5).astype(int)
    if len(set(binary_labels)) > 1:
        auc = roc_auc_score(binary_labels, all_preds)
        ap = average_precision_score(binary_labels, all_preds)
    else:
        auc, ap = 0.0, 0.0
    
    return {
        'auc': auc,
        'ap': ap,
        'predicted_time': predicted_time,
        'confidence': confidence,
        'peak_times': sorted_times[peaks] if len(peaks) > 0 else [],
        'peak_heights': peak_heights if len(peaks) > 0 else [],
        'predictions': sorted_preds,
        'times': sorted_times,
        'labels': sorted_labels
    }

# ✅ 메인 Fine-tuning 함수
def fine_tune_puncture_model(file_dir, base_model_path='./model/best2_model.pth'):
    print("🔧 Starting Focused Puncture Fine-tuning...")
    
    # 모델 로드
    model = LSTMAnomalyDetector(1, 512, 2, 1).to(device)
    
    if os.path.exists(base_model_path):
        model.load_state_dict(torch.load(base_model_path, map_location=device))
        print(f"✅ Base model loaded from {base_model_path}")
    else:
        print("⚠️ Base model not found, starting from scratch")
    
    # Focused Dataset 생성
    train_ds = FocusedPunctureDataset(
        file_dir=file_dir,
        needle_list=['0800', '1000'],
        file_ranges={'train': (1, 17)},
        seq_len=512,
        window=2000  # 더 넓은 윈도우로 컨텍스트 확보
    )
    
    val_ds = FocusedPunctureDataset(
        file_dir=file_dir,
        needle_list=['0800', '1000'],
        file_ranges={'val': (18, 20)},
        seq_len=512,
        window=2000
    )
    
    # DataLoader 생성
    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)
    val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)
    
    # 옵티마이저 및 스케줄러
    optimizer = optim.Adam(model.parameters(), lr=5e-7, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)
    criterion = SharpPunctureLoss(penalty_weight=8.0, focus_weight=15.0)
    
    # Fine-tuning loop
    best_confidence = 0.0
    num_epochs = 10
    
    for epoch in range(1, num_epochs + 1):
        print(f"\n🔄 Fine-tune Epoch {epoch}/{num_epochs}")
        
        # Training
        model.train()
        epoch_loss = 0
        num_batches = 0
        
        for X, y, t in train_dl:
            X, y, t = X.unsqueeze(-1).to(device), y.to(device), t.to(device)
            
            optimizer.zero_grad()
            pred = model(X, t).squeeze()
            loss = criterion(pred, y, t)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        scheduler.step()
        avg_loss = epoch_loss / num_batches
        
        # Validation
        val_results = evaluate_focused_model(model, val_dl, device)
        
        print(f"  Loss: {avg_loss:.6f}")
        print(f"  AUC: {val_results['auc']:.4f}, AP: {val_results['ap']:.4f}")
        
        if val_results['predicted_time'] is not None:
            print(f"  🎯 Predicted puncture time: {val_results['predicted_time']:.0f}")
            print(f"  📊 Confidence: {val_results['confidence']:.4f}")
            
            # 최고 신뢰도 모델 저장
            if val_results['confidence'] > best_confidence:
                best_confidence = val_results['confidence']
                os.makedirs('./model', exist_ok=True)
                torch.save(model.state_dict(), './model/best2_model_finetuned_third.pth')
                print(f"  ✅ Best model saved with confidence: {best_confidence:.4f}")
        
        # 모든 피크 출력
        if len(val_results['peak_times']) > 0:
            peaks_info = [(int(t), f"{h:.3f}") for t, h in 
                         zip(val_results['peak_times'], val_results['peak_heights'])]
            print(f"  📈 All peaks: {peaks_info}")
    
    print(f"\n🎯 Fine-tuning completed! Best confidence: {best_confidence:.4f}")
    
    # 최종 모델 로드 및 평가
    model.load_state_dict(torch.load('./model/best2_model_finetuned_third.pth', map_location=device))
    final_results = evaluate_focused_model(model, val_dl, device)
    
    print(f"\n📊 Final Results:")
    print(f"  AUC: {final_results['auc']:.4f}")
    print(f"  AP: {final_results['ap']:.4f}")
    if final_results['predicted_time'] is not None:
        print(f"  🎯 Final predicted puncture time: {final_results['predicted_time']:.0f}")
        print(f"  📊 Final confidence: {final_results['confidence']:.4f}")
    
    return model, final_results

# ✅ 시각화 함수
def visualize_fine_tuned_results(model, file_dir, needle='1000', file_idx=10):
    """Fine-tuned 모델의 결과를 시각화"""
    fp = os.path.join(file_dir, f'T2D_{needle}', f'SavedData_{file_idx:03d}.bin')
    data = np.fromfile(fp, dtype=np.float32).reshape(-1, 5).T.copy()
    force = data[0]
    norm_force = force - force[0]
    raw_lbl = data[4]
    idxs = np.where(raw_lbl == 1)[0]
    
    if len(idxs) < 2:
        print("No puncture found in this file")
        return
    
    actual_puncture = idxs[1]
    
    # 전체 시퀀스에 대한 예측
    model.eval()
    seq_len = 512
    predictions = []
    times = []
    
    with torch.no_grad():
        for i in range(0, len(norm_force) - seq_len, seq_len // 4):
            X = torch.tensor(norm_force[i:i+seq_len], dtype=torch.float32)
            X = X.unsqueeze(0).unsqueeze(-1).to(device)
            t = torch.tensor([i + seq_len // 2]).to(device)
            
            pred = model(X, t).squeeze()
            pred_prob = torch.sigmoid(pred).cpu().numpy()
            
            predictions.append(pred_prob)
            times.append(i + seq_len // 2)
    
    # 스무딩
    smoothed_preds = gaussian_filter1d(predictions, sigma=5)
    
    # 피크 검출
    peaks, _ = find_peaks(smoothed_preds, height=0.3, distance=50)
    
    # 시각화
    plt.figure(figsize=(15, 10))
    
    # 상단: Force 신호
    plt.subplot(2, 1, 1)
    plt.plot(norm_force, 'b-', label='Normalized Force', alpha=0.7)
    plt.axvline(x=actual_puncture, color='green', linestyle='--', linewidth=2, label=f'Actual Puncture ({actual_puncture})')
    plt.title(f'Force Signal - File {file_idx}')
    plt.xlabel('Time')
    plt.ylabel('Force')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 하단: 예측 결과
    plt.subplot(2, 1, 2)
    plt.plot(times, predictions, 'r-', alpha=0.5, label='Raw Predictions')
    plt.plot(times, smoothed_preds, 'b-', linewidth=2, label='Smoothed Predictions')
    plt.axvline(x=actual_puncture, color='green', linestyle='--', linewidth=2, label=f'Actual Puncture ({actual_puncture})')
    
    # 피크 표시
    if len(peaks) > 0:
        peak_times = [times[p] for p in peaks]
        peak_heights = [smoothed_preds[p] for p in peaks]
        plt.scatter(peak_times, peak_heights, color='red', s=100, zorder=5, label='Detected Peaks')
        
        # 가장 높은 피크 강조
        max_peak_idx = peaks[np.argmax(peak_heights)]
        max_peak_time = times[max_peak_idx]
        plt.scatter([max_peak_time], [smoothed_preds[max_peak_idx]], 
                   color='orange', s=200, zorder=6, label=f'Predicted Puncture ({max_peak_time:.0f})')
    
    plt.title('Fine-tuned Model Predictions')
    plt.xlabel('Time')
    plt.ylabel('Prediction Probability')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('./model/fine_tuned_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 결과 출력
    if len(peaks) > 0:
        max_peak_time = times[peaks[np.argmax(peak_heights)]]
        error = abs(max_peak_time - actual_puncture)
        print(f"\n🎯 Prediction Results:")
        print(f"  Actual puncture time: {actual_puncture}")
        print(f"  Predicted puncture time: {max_peak_time:.0f}")
        print(f"  Error: {error:.0f} samples")
        print(f"  Confidence: {max(peak_heights):.4f}")

# ✅ 실행 코드
if __name__ == "__main__":
    file_dir = '/home/ibom002/dataset/Data_20250709'
    
    # Fine-tuning 실행
    model, results = fine_tune_puncture_model(file_dir)
    
    # 결과 시각화
    visualize_fine_tuned_results(model, file_dir, needle='1000', file_idx=10)
