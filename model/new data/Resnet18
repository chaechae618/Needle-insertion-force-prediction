import os
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
import torch.nn.functional as F

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"  # Arrange GPU devices starting from 0
os.environ["CUDA_VISIBLE_DEVICES"]= "0,1,2"  # Set the GPUs 0,1,2 to use
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('Device:', device)
print('Current cuda device:', torch.cuda.current_device())
print('Count of using GPUs:', torch.cuda.device_count())
torch.cuda.manual_seed_all(42)

# ===================== ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) =====================
def gaussian_kernel1d(kernel_size, sigma):
    # kernel_sizeëŠ” í™€ìˆ˜ì—¬ì•¼ í•¨
    x = torch.arange(kernel_size, dtype=torch.float32) - (kernel_size - 1) / 2.0
    kernel = torch.exp(-0.5 * (x / sigma) ** 2)
    kernel = kernel / kernel.sum()  # í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
    return kernel

def smooth_label_signal(signal, kernel_size=11, sigma=2.0):
    kernel = gaussian_kernel1d(kernel_size, sigma).to(signal.device)
    kernel = kernel.view(1, 1, kernel_size)  # conv1dìš© shape
    padding = kernel_size // 2  # ì…ë ¥ê³¼ ì¶œë ¥ ê¸¸ì´ ë™ì¼í•˜ê²Œ ìœ ì§€
    smoothed_signal = F.conv1d(signal, kernel, padding=padding)
    return smoothed_signal

def replace_zeros_with_random_torch(tensor, low=0.0, high=0.1, seed=42):
    torch.manual_seed(seed)
    mask = tensor <= 0.1
    # maskê°€ Trueì¸ ìœ„ì¹˜ì— ëŒ€í•´ ëœë¤ ê°’ì„ ìƒì„±í•´ì„œ ëŒ€ì…
    random_vals = torch.empty(mask.sum(), device=tensor.device).uniform_(low, high)
    tensor[mask] = random_vals
    return tensor

def Data_preprocessing(needle_dis, seq_len, file_dir, range_list):
    x_seq = []
    y_seq = []
    
    for ind in needle_dis:
        for i in range_list:
            try:
                file_path = os.path.join(file_dir, f'T2D_{ind}/SavedData_{i:03d}.bin')
                with open(file_path, 'rb') as file1:
                    data = np.frombuffer(file1.read(), dtype=np.float32).reshape(-1, 5).T.copy()
                    nor_x = data[0] - data[0][0]
                    
                    # ğŸ” ë””ë²„ê¹…: ì´ˆê¸° ë°ì´í„° ì²´í¬
                    print(f"\n=== {ind}_{i} ì²˜ë¦¬ ì¤‘ ===")
                    print(f"Raw data[0] range: [{data[0].min():.3f}, {data[0].max():.3f}]")
                    print(f"Normalized nor_x range: [{nor_x.min():.3f}, {nor_x.max():.3f}]")
                    
                    # ğŸ” NaN/Inf ì²´í¬
                    if np.isnan(nor_x).any() or np.isinf(nor_x).any():
                        print(f"âš ï¸ Invalid values in nor_x for {ind}_{i}")
                        continue
                        
                    indices = np.where(data[4] == 1)[0]
                    print(f"Puncture indices: {indices}")
                    
                    if len(indices) < 2:
                        print(f"âš ï¸ Less than 2 puncture points found in {ind}_{i}")
                        continue
                    
                    array_size = len(data[4])
                    label_array = np.full(array_size, 0, dtype=np.float32)
                    label_array[indices[1]-20 : indices[1]+20] = 1
                    
                    print(f"Original label_array: min={label_array.min():.3f}, max={label_array.max():.3f}, sum={label_array.sum():.0f}")
                    
                    if ind == '0600':
                        try:
                            # ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                            indices_high = np.where(data[0] > 3.05)[0]
                            indices_low = np.where(data[0] < -3.05)[0]
                            # 100 ì´ìƒ ì°¨ì´ë‚˜ëŠ” ì§€ì  ì°¾ê¸°
                            idx_diff_low = np.where(np.diff(indices_low) > 100)[0]

                            if len(idx_diff_low) > 0 and len(indices_high) > 0:
                                # ì‚­ì œí•  êµ¬ê°„ ì„¤ì •
                                start_1 = indices_high[0]  # ì‹œì‘ êµ¬ê°„ 1
                                end_1 = indices_low[idx_diff_low[0]]  # ë êµ¬ê°„ 1
                                start_2 = indices_low[idx_diff_low[0] + 1]  # ì‹œì‘ êµ¬ê°„ 2
                                end_2 = indices_high[-1]  # ë êµ¬ê°„ 2

                                # start_1 ~ end_2 êµ¬ê°„ì— 6.1ì„ ë”í•˜ê¸°
                                data[0][start_1:end_2 + 1] += 6.1  # í•´ë‹¹ êµ¬ê°„ì— 6.1 ì¶”ê°€

                                # í•„í„°ë§ì„ ìœ„í•œ ë§ˆìŠ¤í¬ ìƒì„±
                                indices_all = np.arange(len(data[0]))  # ì „ì²´ ì¸ë±ìŠ¤ ìƒì„±
                                mask = ~((indices_all >= start_1) & (indices_all <= end_1) | (indices_all >= start_2) & (indices_all <= end_2))

                                # íŠ¹ì • êµ¬ê°„ ì œê±°
                                x_data = data[0][mask]
                                nor_x = x_data - data[0][0]
                                label_array = label_array[mask]
                                
                                print(f"After 0600 processing: data length {len(x_data)}, label length {len(label_array)}")
                        except Exception as e:
                            print(f"âš ï¸ ì˜ˆì™¸ ë°œìƒ (ind: {ind}, i: {i}) - {e}")
                            pass  # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                    
                    # ğŸ” ê¸¸ì´ ì²´í¬
                    if len(nor_x) == 0:
                        print(f"âš ï¸ Empty data after processing {ind}_{i}")
                        continue
                    
                    # ğŸ” ë ˆì´ë¸” ì²´í¬
                    if np.all(label_array == 0):
                        print(f"âš ï¸ All zero labels for {ind}_{i}")
                        # ìµœì†Œí•œì˜ ë ˆì´ë¸” ìƒì„±
                        mid_point = len(label_array) // 2
                        label_array[mid_point-10:mid_point+10] = 0.1
                    
                    # ìŠ¤ë¬´ë”© ì ìš©
                    label_tensor = torch.tensor(label_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                    smoothed_signal = smooth_label_signal(label_tensor, kernel_size=10001, sigma=500.0)
                    smoothed_signal = smoothed_signal.squeeze(0).squeeze(0)
                    
                    print(f"After smoothing: min={smoothed_signal.min():.6f}, max={smoothed_signal.max():.6f}")
                    
                    # ğŸ”§ ì•ˆì „í•œ ì •ê·œí™”
                    max_val = smoothed_signal.max()
                    if max_val > 1e-6:  # ë§¤ìš° ì‘ì€ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì •ê·œí™”
                        smoothed_signal = smoothed_signal / max_val
                    else:
                        print(f"âš ï¸ Max value too small ({max_val:.2e}), skipping normalization")
                        smoothed_signal = torch.ones_like(smoothed_signal) * 0.01
                    
                    print(f"After normalization: min={smoothed_signal.min():.6f}, max={smoothed_signal.max():.6f}")
                    
                    # ëœë¤ ê°’ ëŒ€ì²´
                    smoothed_signal = replace_zeros_with_random_torch(smoothed_signal, low=0.01, high=0.1)
                    
                    print(f"After random replace: min={smoothed_signal.min():.6f}, max={smoothed_signal.max():.6f}")
                    
                    # ğŸ”§ ìµœì¢… ì•ˆì „ì„± ì²´í¬ ë° í´ë¦¬í•‘
                    if torch.isnan(smoothed_signal).any():
                        print("âš ï¸ NaN detected in smoothed_signal!")
                        continue
                    if torch.isinf(smoothed_signal).any():
                        print("âš ï¸ Inf detected in smoothed_signal!")
                        continue
                    
                    # BCE ë²”ìœ„ë¡œ í´ë¦¬í•‘
                    smoothed_signal = torch.clamp(smoothed_signal, 0.0, 1.0)
                    print(f"After clipping: min={smoothed_signal.min():.6f}, max={smoothed_signal.max():.6f}")
                    
                    # ì‹œí€€ìŠ¤ ìƒì„±
                    valid_count = 0
                    for j in range(seq_len, len(nor_x) - seq_len):
                        x_win = nor_x[j - seq_len : j]
                        
                        # ğŸ” ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ì²´í¬
                        if np.isnan(x_win).any() or np.isinf(x_win).any():
                            continue
                        
                        y_win = smoothed_signal[j]
                        
                        # ğŸ” ë ˆì´ë¸” ìœ íš¨ì„± ì²´í¬  
                        if torch.isnan(y_win) or torch.isinf(y_win):
                            continue
                        
                        x_seq.append(torch.tensor(x_win, dtype=torch.float32))
                        y_seq.append(torch.tensor(y_win.item(), dtype=torch.float32))  # .item()ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ ë³€í™˜
                        valid_count += 1
                    
                    print(f'{ind}_{i} file processed. Data length: {len(nor_x)}, Valid sequences: {valid_count}')
                    
            except Exception as e:
                print(f"âš ï¸ Error processing {ind}_{i}: {e}")
                continue
    
    print(f'Total sequences: {len(x_seq)}')
    return x_seq, y_seq

# ===================== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼) =====================
class train_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0800', '1000']  # 0600 ì œê±°
        self.seq_len = seq_len
        self.x_seq = []
        self.y_seq = []
        self.range_list = [i for i in range(1, 19)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]               
    
class val_bin_dataset(Dataset):
    def __init__(self, file_dir, seq_len):
        self.needle_dis = ['0800', '1000']  # 0600 ì œê±°
        self.seq_len = seq_len
        self.x_seq = []
        self.y_seq = []
        self.range_list = [i for i in range(18, 21)]
        self.file_dir = file_dir
        
        self.x_seq, self.y_seq = Data_preprocessing(self.needle_dis, self.seq_len, file_dir, self.range_list)
        
    def __len__(self):
        return len(self.y_seq)
    
    def __getitem__(self, idx):
        return self.x_seq[idx], self.y_seq[idx]

# ===================== ResNet1D ëª¨ë¸ ì •ì˜ =====================
class BasicBlock1D(nn.Module):
    """ResNetì˜ ê¸°ë³¸ ë¸”ë¡ì„ 1Dë¡œ ë³€í™˜"""
    expansion = 1
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock1D, self).__init__()
        
        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm1d(out_channels)
        
        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, 
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        
    def forward(self, x):
        identity = x
        
        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
        out = self.conv2(out)
        out = self.bn2(out)
        
        # Skip connection
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        return out

class ResNet1D(nn.Module):
    """1D ResNet for Puncture Detection"""
    def __init__(self, block, layers, num_classes=1, input_channels=1):
        super(ResNet1D, self).__init__()
        
        self.in_channels = 64
        
        # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, 
                              stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm1d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)
        
        # ResNet ë¸”ë¡ë“¤
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.fc = nn.Sequential(
            nn.Linear(512 * block.expansion, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv1d(self.in_channels, out_channels * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm1d(out_channels * block.expansion),
            )
        
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ì…ë ¥ ì°¨ì› í™•ì¸ ë° ì¡°ì •: [batch_size, seq_len] -> [batch_size, 1, seq_len]
        if len(x.shape) == 2:
            x = x.unsqueeze(1)
        
        # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        # ResNet ë¸”ë¡ë“¤
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        
        # ë¶„ë¥˜
        x = self.fc(x)
        
        return x.squeeze(-1)  # [batch_size, 1] -> [batch_size]

def resnet18_1d(num_classes=1, input_channels=1):
    """ResNet-18ì„ 1Dë¡œ ë³€í™˜"""
    return ResNet1D(BasicBlock1D, [2, 2, 2, 2], num_classes, input_channels)

def resnet34_1d(num_classes=1, input_channels=1):
    """ResNet-34ë¥¼ 1Dë¡œ ë³€í™˜"""
    return ResNet1D(BasicBlock1D, [3, 4, 6, 3], num_classes, input_channels)

# ===================== í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ =====================
def train_epoch(train_dl, model, criterion, optimizer):
   """í•œ ì—í¬í¬ í•™ìŠµ"""
   model.train()
   epoch_loss = 0
   total_batches = len(train_dl)
   
   for batch_idx, (X, y) in enumerate(train_dl):
       optimizer.zero_grad()
       
       # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
       X = X.to(device)  # [batch_size, seq_len]
       y = y.to(device).float()  # [batch_size]
       
       # Forward pass
       pred = model(X)  # [batch_size]
       loss = criterion(pred, y)
       
       # Backward pass
       loss.backward()
       
       # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì„ íƒì‚¬í•­)
       torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
       
       optimizer.step()
       
       epoch_loss += loss.item()
   
   return epoch_loss / total_batches

def validate_epoch(val_dl, model, criterion):
    """ê²€ì¦ ì—í¬í¬"""
    model.eval()
    epoch_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for X, y in val_dl:
            X = X.to(device)
            y = y.to(device).float()
            
            # Forward pass
            pred = model(X)
            loss = criterion(pred, y)
            
            epoch_loss += loss.item()
            
            # í™•ë¥ ë¡œ ë³€í™˜
            probas = torch.sigmoid(pred)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            all_preds.extend(probas.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_loss = epoch_loss / len(val_dl)
    
    # ì´ì§„ ë¶„ë¥˜ AUC ê³„ì‚°
    try:
        binary_labels = [1 if label > 0.8 else 0 for label in all_labels]
        if len(set(binary_labels)) > 1:
            auc_score = roc_auc_score(binary_labels, all_preds)
        else:
            auc_score = 0.0
    except Exception as e:
        print(f"âš ï¸ AUC ê³„ì‚° ì˜¤ë¥˜: {e}")
        auc_score = 0.0
    
    return avg_loss, auc_score, all_preds, all_labels

# ===================== ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ =====================
if __name__ == "__main__":
    print("ğŸš€ ResNet1D ê¸°ë°˜ Puncture Detection ì‹œì‘!")
    print("="*60)
    
    # ë°ì´í„° ë¡œë”©
    file_dir = '/home/ibom002/dataset/Data_20250709'
    seq_len = 512
    batch_size = 256  # ResNetì€ LSTMë³´ë‹¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    train_ds = train_bin_dataset(file_dir, seq_len)
    val_ds = val_bin_dataset(file_dir, seq_len)
    
    print(f'Train dataset size: {len(train_ds)}')
    print(f'Validation dataset size: {len(val_ds)}')
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ ìƒ˜í”Œë§
    binary_labels = [1 if y.item() > 0.8 else 0 for _, y in train_ds]
    class_counts = [binary_labels.count(0), binary_labels.count(1)]
    class_weights = [1.0 / class_counts[0], 1.0 / class_counts[1]]
    
    print(f"í´ë˜ìŠ¤ ë¶„í¬ - ìŒì„±: {class_counts[0]}, ì–‘ì„±: {class_counts[1]}")
    print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ - ìŒì„±: {class_weights[0]:.4f}, ì–‘ì„±: {class_weights[1]:.4f}")
    
    # ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì„¤ì •
    weights = [class_weights[label] for label in binary_labels]
    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, 
                         shuffle=False, drop_last=True, num_workers=4)
    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, 
                       drop_last=True, num_workers=4)
    
    print(f'Train batches: {len(train_dl)}')
    print(f'Validation batches: {len(val_dl)}')
    
    # ëª¨ë¸ ìƒì„±
    print("\nëª¨ë¸ ìƒì„± ì¤‘...")
    model = resnet18_1d(num_classes=1, input_channels=1)
    
    # Multi-GPU ì„¤ì •
    if torch.cuda.device_count() > 1:
        print(f"ğŸš€ Using {torch.cuda.device_count()} GPUs with DataParallel!")
        model = nn.DataParallel(model)
    
    model = model.to(device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    
    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    criterion = nn.BCEWithLogitsLoss()
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3)
    
    # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
    patience = 7
    best_val_loss = float('inf')
    early_stop_counter = 0
    min_delta = 0.0001
    
    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    train_losses = []
    val_losses = []
    val_aucs = []
    
    # í•™ìŠµ ì‹œì‘
    print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
    print("="*60)
    
    num_epochs = 50
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“ Epoch {epoch+1}/{num_epochs}")
        print("-" * 40)
        
        # í•™ìŠµ
        train_loss = train_epoch(train_dl, model, criterion, optimizer)
        print(f"í›ˆë ¨ ì†ì‹¤: {train_loss:.6f}")
        
        # ê²€ì¦
        val_loss, val_auc, val_preds, val_labels = validate_epoch(val_dl, model, criterion)
        print(f"ê²€ì¦ ì†ì‹¤: {val_loss:.6f}")
        print(f"ê²€ì¦ AUC: {val_auc:.5f}")
        
        # ê²°ê³¼ ì €ì¥
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_aucs.append(val_auc)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        print(f"í˜„ì¬ í•™ìŠµë¥ : {current_lr:.2e}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if val_loss < (best_val_loss - min_delta):
            best_val_loss = val_loss
            early_stop_counter = 0
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            model_save_path = "./model/best_resnet1d_model.pth"
            os.makedirs("./model", exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
        else:
            early_stop_counter += 1
            print(f"âš ï¸ ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°: {early_stop_counter}/{patience}")
        
        if early_stop_counter >= patience:
            print("â¹ ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”!")
            break
    
    print("\nğŸ¯ í•™ìŠµ ì™„ë£Œ!")
    print(f"ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
    print(f"ìµœê³  ê²€ì¦ AUC: {max(val_aucs):.5f}")
    
